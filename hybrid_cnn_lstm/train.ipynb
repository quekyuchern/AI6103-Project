{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.datasets import fetch_20newsgroups\nimport torch\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\n# Automatically download missing NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Check CUDA availability and set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom print functions\ndef print_timer_info(message):\n    print(f\"[TIMER INFO] {message}\")\n\ndef print_output_data(message):\n    print(f\"[OUTPUT DATA] {message}\")\n\n# Load the dataset\nstart_time = time.time()\nprint_timer_info(\"Loading the 20 Newsgroups dataset...\")\nnewsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\ndf = pd.DataFrame({'Text': newsgroups.data, 'Category': newsgroups.target})\ndf['Category Name'] = df['Category'].apply(lambda x: newsgroups.target_names[x])\nprint_timer_info(f\"Time taken to load dataset: {time.time() - start_time:.2f} seconds\")\n\n# Pre-processing\nprint_timer_info(\"Starting pre-processing...\")\npreprocess_start_time = time.time()\n\ndf['Text'] = df['Text'].str.lower()  # Lowercasing\ndf['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove punctuation/special characters\ndf['Tokens'] = df['Text'].apply(word_tokenize)  # Tokenization\n\nstop_words = set(stopwords.words('english'))\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])  # Stopwords removal\n\nstemmer = PorterStemmer()\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])  # Stemming\n\nprint_timer_info(f\"Total time for pre-processing: {time.time() - preprocess_start_time:.2f} seconds\")\n\n# Move processed data to GPU if available (Example - converting text data to tensor)\nmax_token_length = max(df['Tokens'].apply(len))\nprint_output_data(f\"Maximum length of tokenized text: {max_token_length}\")\n\n# Data Preparation\ndata_prep_start_time = time.time()\nprint_timer_info(\"Starting data preparation...\")\n\n# Define and fit the TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=6620, stop_words='english')  # Set max_features to limit vocab size if necessary\nX_tfidf = tfidf_vectorizer.fit_transform(df['Text']).toarray()  # Assuming you are using the original 'Text' column\n\n# Encode the labels\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['Category'])\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create PyTorch Dataset\nclass TfidfDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_dataset = TfidfDataset(X_train_tensor, y_train_tensor)\nval_dataset = TfidfDataset(X_val_tensor, y_val_tensor)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint_timer_info(f\"Total time for data preparation: {time.time() - data_prep_start_time:.2f} seconds\")\n","metadata":{"ExecuteTime":{"end_time":"2024-11-04T14:46:37.187204Z","start_time":"2024-11-04T14:46:07.103185Z"},"execution":{"iopub.status.busy":"2024-11-04T21:12:02.004113Z","iopub.execute_input":"2024-11-04T21:12:02.004555Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[TIMER INFO] Loading the 20 Newsgroups dataset...\n[TIMER INFO] Time taken to load dataset: 13.14 seconds\n[TIMER INFO] Starting pre-processing...\n","output_type":"stream"}]}]}