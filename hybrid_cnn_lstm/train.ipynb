{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import Counter\n\n# Helper function for timing info\ndef print_timer_info(message):\n    print(f\"[TIMER INFO] {message}\")\n\n# Set up device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint_timer_info(f\"Using device: {device}\")\n\n# === Load Dataset ===\nstart_time = time.time()\nprint_timer_info(\"Loading the 20 Newsgroups dataset...\")\nnewsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\ndf = pd.DataFrame({'Text': newsgroups.data, 'Category': newsgroups.target})\nprint_timer_info(f\"Dataset loaded in {time.time() - start_time:.2f} seconds\")\n\n# === Pre-process Data ===\nprint_timer_info(\"Starting data pre-processing...\")\npreprocess_start_time = time.time()\n\n# Text cleaning, tokenization, stopword removal, and stemming\ndf['Text'] = df['Text'].str.lower()\ndf['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\ndf['Tokens'] = df['Text'].apply(word_tokenize)\n\n# Stopwords removal\nnltk.download('stopwords', quiet=True)\nstop_words = set(stopwords.words('english'))\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n# Stemming\nstemmer = PorterStemmer()\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\nprint_timer_info(f\"Pre-processing completed in {time.time() - preprocess_start_time:.2f} seconds\")\n\n# === Vocabulary and Sequence Preparation ===\nprint_timer_info(\"Building vocabulary and preparing sequences...\")\nvocab_build_start_time = time.time()\n\n# Build vocabulary\nvocab = Counter()\nfor tokens in df['Tokens']:\n    vocab.update(tokens)\nvocab_size = 10000\nvocab = dict(vocab.most_common(vocab_size))\nword_to_index = {word: idx + 1 for idx, word in enumerate(vocab.keys())}\n\n# Random embedding matrix initialization with increased dimension\nembedding_dim = 200\nembedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size + 1, embedding_dim))\nembedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32).to(device)\nprint_timer_info(f\"Vocabulary and embedding matrix created in {time.time() - vocab_build_start_time:.2f} seconds\")\n\n# Convert text to sequences and pad sequences\nmax_seq_length = 120\ndf['Sequences'] = df['Tokens'].apply(lambda tokens: [word_to_index.get(word, 0) for word in tokens])\ndf['Padded_Sequences'] = df['Sequences'].apply(lambda seq: seq[:max_seq_length] + [0] * (max_seq_length - len(seq)))\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['Category'])\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(df['Padded_Sequences'], y, test_size=0.2, random_state=42)\n\n# Convert to tensors\nX_train_tensor = torch.tensor(X_train.tolist(), dtype=torch.long)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val.tolist(), dtype=torch.long)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create PyTorch Dataset\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Create DataLoaders with reduced batch size\nbatch_size = 16\ntrain_dataset = SequenceDataset(X_train_tensor, y_train_tensor)\nval_dataset = SequenceDataset(X_val_tensor, y_val_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint_timer_info(\"Data preparation completed\")\n\n# === Define CNN-LSTM Model with Dropout Regularization ===\nclass CNNLSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_lstm_layers=2, kernel_size=5, num_filters=128, dropout_prob=0.5):\n        super(CNNLSTMClassifier, self).__init__()\n        \n        # Embedding layer with random initialization, CNN, LSTM, and fully connected layers with dropout\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.lstm = nn.LSTM(input_size=num_filters, hidden_size=hidden_dim, num_layers=num_lstm_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = x.permute(0, 2, 1)\n        x, _ = self.lstm(x)\n        x = x[:, -1, :]\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n# Model Parameters\nhidden_dim = 256\noutput_dim = len(label_encoder.classes_)\n\n# Instantiate and move model to device\nmodel = CNNLSTMClassifier(vocab_size=vocab_size + 1, embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# === Train Model with Early Stopping ===\nnum_epochs = 100\npatience = 10\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\nprint_timer_info(\"Starting model training...\")\ntraining_start_time = time.time()\n\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n    model.train()\n    epoch_loss = 0\n\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        predictions = model(X_batch)\n        loss = criterion(predictions, y_batch)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    # Validation phase\n    model.eval()\n    val_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            predictions = model(X_batch)\n            loss = criterion(predictions, y_batch)\n            val_loss += loss.item()\n            _, predicted_classes = torch.max(predictions, 1)\n            correct_predictions += (predicted_classes == y_batch).sum().item()\n            total_predictions += y_batch.size(0)\n\n    val_acc = correct_predictions / total_predictions\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"[EPOCH {epoch + 1}/{num_epochs}] Training Loss: {epoch_loss / len(train_loader):.4f}, \"\n          f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}, \"\n          f\"Epoch Time: {time.time() - epoch_start_time:.2f} seconds\")\n\n    # Early stopping\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        best_model_state = model.state_dict()\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print_timer_info(f\"Early stopping triggered after {epoch + 1} epochs\")\n        break\n\nprint_timer_info(f\"Total training time: {time.time() - training_start_time:.2f} seconds\")\n\n# Load best model state\nmodel.load_state_dict(best_model_state)\n\n# === Evaluate Model ===\nprint_timer_info(\"Evaluating model on validation set...\")\neval_start_time = time.time()\n\n# Evaluation metrics\nmodel.eval()\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        outputs = model(X_batch)\n        _, predicted = torch.max(outputs, 1)\n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.cpu().numpy())\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(all_labels, all_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n\nprint(f\"\\n=== Evaluation Results ===\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint_timer_info(f\"Evaluation completed in {time.time() - eval_start_time:.2f} seconds\")\n\n# === Classification Report ===\nprint_timer_info(\"Generating classification report...\")\nreport_start_time = time.time()\nreport = classification_report(all_labels, all_predictions, target_names=list(map(str, label_encoder.classes_)))\nprint(report)\nprint_timer_info(f\"Classification report generated in {time.time() - report_start_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:43:39.694156Z","iopub.execute_input":"2024-11-06T11:43:39.694554Z","iopub.status.idle":"2024-11-06T11:48:07.318264Z","shell.execute_reply.started":"2024-11-06T11:43:39.694517Z","shell.execute_reply":"2024-11-06T11:48:07.317321Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[TIMER INFO] Using device: cuda\n[TIMER INFO] Loading the 20 Newsgroups dataset...\n[TIMER INFO] Dataset loaded in 2.96 seconds\n[TIMER INFO] Starting data pre-processing...\n[TIMER INFO] Pre-processing completed in 85.44 seconds\n[TIMER INFO] Building vocabulary and preparing sequences...\n[TIMER INFO] Vocabulary and embedding matrix created in 0.43 seconds\n[TIMER INFO] Data preparation completed\n[TIMER INFO] Starting model training...\n[EPOCH 2/100] Training Loss: 2.5411, Validation Loss: 2.4478, Validation Accuracy: 0.1554, Epoch Time: 7.46 seconds\n[EPOCH 3/100] Training Loss: 2.3646, Validation Loss: 2.3198, Validation Accuracy: 0.1732, Epoch Time: 7.32 seconds\n[EPOCH 4/100] Training Loss: 2.2260, Validation Loss: 2.2464, Validation Accuracy: 0.1984, Epoch Time: 7.36 seconds\n[EPOCH 5/100] Training Loss: 2.0807, Validation Loss: 2.1431, Validation Accuracy: 0.2146, Epoch Time: 7.62 seconds\n[EPOCH 6/100] Training Loss: 1.9364, Validation Loss: 2.0460, Validation Accuracy: 0.2676, Epoch Time: 7.32 seconds\n[EPOCH 7/100] Training Loss: 1.8121, Validation Loss: 2.0156, Validation Accuracy: 0.2854, Epoch Time: 7.33 seconds\n[EPOCH 8/100] Training Loss: 1.6877, Validation Loss: 1.8871, Validation Accuracy: 0.3398, Epoch Time: 7.33 seconds\n[EPOCH 23/100] Training Loss: 0.6940, Validation Loss: 1.7132, Validation Accuracy: 0.5443, Epoch Time: 7.34 seconds\n[EPOCH 24/100] Training Loss: 0.6688, Validation Loss: 1.7886, Validation Accuracy: 0.5377, Epoch Time: 7.32 seconds\n[TIMER INFO] Early stopping triggered after 24 epochs\n[TIMER INFO] Total training time: 177.11 seconds\n[TIMER INFO] Evaluating model on validation set...\n\n=== Evaluation Results ===\nAccuracy: 0.5377\nPrecision: 0.5563\nRecall: 0.5377\nF1 Score: 0.5386\n[TIMER INFO] Evaluation completed in 0.54 seconds\n[TIMER INFO] Generating classification report...\n              precision    recall  f1-score   support\n\n           0       0.35      0.46      0.39       151\n           1       0.46      0.46      0.46       202\n           2       0.61      0.37      0.46       195\n           3       0.47      0.45      0.46       183\n           4       0.42      0.55      0.47       205\n           5       0.92      0.57      0.70       215\n           6       0.59      0.56      0.57       193\n           7       0.40      0.61      0.48       196\n           8       0.56      0.65      0.60       168\n           9       0.70      0.67      0.68       211\n          10       0.82      0.73      0.78       198\n          11       0.64      0.67      0.65       201\n          12       0.45      0.44      0.45       202\n          13       0.71      0.57      0.63       194\n          14       0.43      0.62      0.51       189\n          15       0.68      0.63      0.65       202\n          16       0.45      0.51      0.48       188\n          17       0.84      0.67      0.74       182\n          18       0.28      0.31      0.30       159\n          19       0.10      0.04      0.05       136\n\n    accuracy                           0.54      3770\n   macro avg       0.54      0.53      0.53      3770\nweighted avg       0.56      0.54      0.54      3770\n\n[TIMER INFO] Classification report generated in 0.01 seconds\n","output_type":"stream"}]}]}