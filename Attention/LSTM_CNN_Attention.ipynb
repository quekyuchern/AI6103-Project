{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9750129,"sourceType":"datasetVersion","datasetId":5969306}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import joblib\n\nX_train, y_train, X_test, y_test = joblib.load('/kaggle/input/bert-emb/bert_train_test_data.pkl')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-14T12:50:47.969365Z","iopub.execute_input":"2024-11-14T12:50:47.970080Z","iopub.status.idle":"2024-11-14T12:51:29.609291Z","shell.execute_reply.started":"2024-11-14T12:50:47.970036Z","shell.execute_reply":"2024-11-14T12:51:29.608383Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, Bidirectional, LSTM, Input, LayerNormalization, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Best model parameters\nmax_sequence_length, embedding_dim = 128, 768\nfixed_lstm_units = 256  # LSTM units fixed at 256\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the fixed best parameters\ninputs = Input(shape=(max_sequence_length, embedding_dim))\nx = Bidirectional(LSTM(fixed_lstm_units, return_sequences=True))(inputs)\nx = Dropout(0.2)(x)\n\n# Multi-head attention layer\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(x, x)\nattention_output = LayerNormalization()(attention_output + x)\n\n# Global Average Pooling\npooled_output = GlobalAveragePooling1D()(attention_output)\n\n# Dense layers\nx = Dense(64, activation='relu')(pooled_output)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\noutputs = Dense(num_classes, activation='softmax')(x)\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,  # Limit epochs to 100 to reduce time and memory usage\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the best model configuration: LSTM Units=256, Attention Heads=16, First Decay Steps=40\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T14:14:04.096705Z","iopub.execute_input":"2024-11-11T14:14:04.097133Z","iopub.status.idle":"2024-11-11T14:26:50.322853Z","shell.execute_reply.started":"2024-11-11T14:14:04.097099Z","shell.execute_reply":"2024-11-11T14:26:50.321875Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - accuracy: 0.1595 - loss: 2.6940 - val_accuracy: 0.5419 - val_loss: 1.4889\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.4948 - loss: 1.6183 - val_accuracy: 0.6340 - val_loss: 1.2000\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.6116 - loss: 1.2631 - val_accuracy: 0.6257 - val_loss: 1.2078\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.6266 - loss: 1.2214 - val_accuracy: 0.6687 - val_loss: 1.0729\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.7020 - loss: 1.0054 - val_accuracy: 0.6889 - val_loss: 1.0195\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.7110 - loss: 0.9436 - val_accuracy: 0.6740 - val_loss: 1.1156\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.6952 - loss: 0.9885 - val_accuracy: 0.6698 - val_loss: 1.1037\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.7359 - loss: 0.8608 - val_accuracy: 0.7011 - val_loss: 1.0355\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.7851 - loss: 0.7056 - val_accuracy: 0.7045 - val_loss: 1.0706\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.8192 - loss: 0.5679 - val_accuracy: 0.7093 - val_loss: 1.1025\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.8404 - loss: 0.5117 - val_accuracy: 0.6849 - val_loss: 1.1340\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.7652 - loss: 0.7549 - val_accuracy: 0.6878 - val_loss: 1.1082\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.7948 - loss: 0.6755 - val_accuracy: 0.6886 - val_loss: 1.1222\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8239 - loss: 0.5713 - val_accuracy: 0.6997 - val_loss: 1.1527\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8552 - loss: 0.4633 - val_accuracy: 0.7034 - val_loss: 1.2825\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8829 - loss: 0.3843 - val_accuracy: 0.7093 - val_loss: 1.3163\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9025 - loss: 0.3245 - val_accuracy: 0.7069 - val_loss: 1.4167\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9120 - loss: 0.2873 - val_accuracy: 0.7162 - val_loss: 1.4848\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9302 - loss: 0.2222 - val_accuracy: 0.7210 - val_loss: 1.5868\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9341 - loss: 0.2118 - val_accuracy: 0.7204 - val_loss: 1.6847\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9406 - loss: 0.1913 - val_accuracy: 0.7223 - val_loss: 1.7180\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9383 - loss: 0.2024 - val_accuracy: 0.6729 - val_loss: 1.6301\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.8458 - loss: 0.5076 - val_accuracy: 0.6979 - val_loss: 1.4972\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8730 - loss: 0.4241 - val_accuracy: 0.6987 - val_loss: 1.4834\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9014 - loss: 0.3257 - val_accuracy: 0.7050 - val_loss: 1.5876\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9153 - loss: 0.2886 - val_accuracy: 0.7106 - val_loss: 1.4898\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9226 - loss: 0.2549 - val_accuracy: 0.7024 - val_loss: 1.7564\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9342 - loss: 0.2192 - val_accuracy: 0.7050 - val_loss: 1.7230\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9390 - loss: 0.2157 - val_accuracy: 0.7008 - val_loss: 1.9032\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9411 - loss: 0.1936 - val_accuracy: 0.7111 - val_loss: 1.9210\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9473 - loss: 0.1732 - val_accuracy: 0.7154 - val_loss: 1.9423\nCompleted training with the best model configuration: LSTM Units=256, Attention Heads=16, First Decay Steps=40\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-11T14:26:50.324206Z","iopub.execute_input":"2024-11-11T14:26:50.324764Z","iopub.status.idle":"2024-11-11T14:26:56.312213Z","shell.execute_reply.started":"2024-11-11T14:26:50.324726Z","shell.execute_reply":"2024-11-11T14:26:56.311256Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7183 - loss: 1.7675\nTest Loss: 1.7180156707763672\nTest Accuracy: 0.7222811579704285\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CNN-LSTM","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MultiHeadAttention, Dense, Dropout, Bidirectional, LSTM, Input, LayerNormalization, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nfixed_lstm_units = 256  # Best LSTM units\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with CNN layer before LSTM and added regularization\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# CNN layer with L2 regularization\nx = Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=l2(0.01))(inputs)\nx = Dropout(0.3)(x)  # Dropout after CNN to prevent overfitting\n\n# Bidirectional LSTM with L2 regularization\nx = Bidirectional(LSTM(fixed_lstm_units, return_sequences=True, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))(x)\nx = Dropout(0.3)(x)  # Dropout after LSTM\n\n# Multi-head attention layer with LayerNormalization\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(x, x)\nattention_output = LayerNormalization()(attention_output + x)\n\n# Global Average Pooling\npooled_output = GlobalAveragePooling1D()(attention_output)\n\n# Dense layers with L2 regularization and Dropout\nx = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(pooled_output)\nx = Dropout(0.3)(x)\nx = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\nx = Dropout(0.3)(x)\noutputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,  # Limit epochs to 100 to reduce time and memory usage\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with CNN + LSTM + Attention model with regularization\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T15:03:55.353564Z","iopub.execute_input":"2024-11-11T15:03:55.353996Z","iopub.status.idle":"2024-11-11T15:15:26.286201Z","shell.execute_reply.started":"2024-11-11T15:03:55.353949Z","shell.execute_reply":"2024-11-11T15:15:26.285200Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - accuracy: 0.0522 - loss: 9.5758 - val_accuracy: 0.1546 - val_loss: 3.1628\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.1748 - loss: 2.9743 - val_accuracy: 0.4520 - val_loss: 1.9822\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4087 - loss: 2.1039 - val_accuracy: 0.4859 - val_loss: 1.8979\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4515 - loss: 2.0071 - val_accuracy: 0.5310 - val_loss: 1.7241\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5268 - loss: 1.7485 - val_accuracy: 0.5708 - val_loss: 1.6081\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5410 - loss: 1.7018 - val_accuracy: 0.5130 - val_loss: 1.8023\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5258 - loss: 1.7760 - val_accuracy: 0.5584 - val_loss: 1.6574\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5477 - loss: 1.7111 - val_accuracy: 0.5719 - val_loss: 1.6039\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5891 - loss: 1.5538 - val_accuracy: 0.5968 - val_loss: 1.5233\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6110 - loss: 1.4857 - val_accuracy: 0.6138 - val_loss: 1.4991\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6228 - loss: 1.4638 - val_accuracy: 0.5631 - val_loss: 1.6359\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.5666 - loss: 1.6358 - val_accuracy: 0.5618 - val_loss: 1.6316\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5812 - loss: 1.6142 - val_accuracy: 0.5910 - val_loss: 1.5719\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6074 - loss: 1.5423 - val_accuracy: 0.6003 - val_loss: 1.5525\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6188 - loss: 1.5072 - val_accuracy: 0.6008 - val_loss: 1.5409\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6237 - loss: 1.4713 - val_accuracy: 0.6156 - val_loss: 1.5242\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6428 - loss: 1.4144 - val_accuracy: 0.6236 - val_loss: 1.4918\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6621 - loss: 1.3651 - val_accuracy: 0.6149 - val_loss: 1.4920\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6739 - loss: 1.3296 - val_accuracy: 0.6398 - val_loss: 1.4759\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6907 - loss: 1.2512 - val_accuracy: 0.6369 - val_loss: 1.4662\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6995 - loss: 1.2387 - val_accuracy: 0.6459 - val_loss: 1.4631\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6973 - loss: 1.2555 - val_accuracy: 0.6050 - val_loss: 1.5505\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6331 - loss: 1.4826 - val_accuracy: 0.5674 - val_loss: 1.6556\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6337 - loss: 1.4995 - val_accuracy: 0.6093 - val_loss: 1.5406\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6407 - loss: 1.4738 - val_accuracy: 0.6077 - val_loss: 1.5568\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6214 - loss: 1.5118 - val_accuracy: 0.6233 - val_loss: 1.5407\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6452 - loss: 1.4565 - val_accuracy: 0.6279 - val_loss: 1.4956\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6477 - loss: 1.4490 - val_accuracy: 0.6016 - val_loss: 1.5972\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6609 - loss: 1.4224 - val_accuracy: 0.6345 - val_loss: 1.5053\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6669 - loss: 1.3889 - val_accuracy: 0.6194 - val_loss: 1.5705\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6685 - loss: 1.3828 - val_accuracy: 0.6424 - val_loss: 1.5112\nCompleted training with CNN + LSTM + Attention model with regularization\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-11T15:15:26.297225Z","iopub.execute_input":"2024-11-11T15:15:26.297539Z","iopub.status.idle":"2024-11-11T15:15:32.064786Z","shell.execute_reply.started":"2024-11-11T15:15:26.297505Z","shell.execute_reply":"2024-11-11T15:15:32.063870Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6377 - loss: 1.4929\nTest Loss: 1.463086724281311\nTest Accuracy: 0.6458885669708252\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MultiHeadAttention, Dense, Dropout, Bidirectional, LSTM, Input, LayerNormalization, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nimport itertools\nimport json\nimport gc\nimport os\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Fixed values\ndropout_rate = 0.3\nlearning_rate = 0.0005\n\n# Define hyperparameter options for grid search\nlstm_units_options = [128, 256]\ncnn_filters_options = [64, 128, 256]\nkernel_size_options = [3, 5]\nattention_heads_options = [8, 16]\n\n# File to store results incrementally\nresults_file = \"/kaggle/working/model_results.json\"\n\n# Check if results file exists and load existing results if present\nif os.path.exists(results_file):\n    with open(results_file, 'r') as f:\n        results = json.load(f)\nelse:\n    results = []\n\n# Grid search with memory management\nfor lstm_units, cnn_filters, kernel_size, attention_heads in itertools.product(\n        lstm_units_options, cnn_filters_options, kernel_size_options, attention_heads_options):\n\n    # Define the model with current hyperparameters\n    inputs = Input(shape=(max_sequence_length, embedding_dim))\n    \n    # LSTM layer with bidirectional configuration\n    x = Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))(inputs)\n    x = Dropout(dropout_rate)(x)\n\n    # CNN layer with current filter and kernel size\n    x = Conv1D(filters=cnn_filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n    x = Dropout(dropout_rate)(x)\n\n    # Multi-head attention layer\n    attention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(x, x)\n    attention_output = LayerNormalization()(attention_output + x)\n\n    # Global Average Pooling\n    pooled_output = GlobalAveragePooling1D()(attention_output)\n\n    # Dense layers with dropout\n    x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(pooled_output)\n    x = Dropout(dropout_rate)(x)\n    x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\n    x = Dropout(dropout_rate)(x)\n    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)\n\n    # Define the model\n    model = Model(inputs=inputs, outputs=outputs)\n\n    # Cosine annealing learning rate schedule\n    cosine_annealing = CosineDecayRestarts(\n        initial_learning_rate=learning_rate,\n        first_decay_steps=40,\n        t_mul=2,\n        alpha=0.01\n    )\n    optimizer = Adam(learning_rate=cosine_annealing)\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    # Early stopping\n    early_stopping = EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        X_train, y_train,\n        epochs=100,\n        validation_data=(X_test, y_test),\n        callbacks=[early_stopping],\n        verbose=1\n    )\n\n    # Convert history values to 32-bit floats to save memory\n    history_data = {\n        'train_loss': [float(np.float32(loss)) for loss in history.history['loss']],\n        'train_accuracy': [float(np.float32(acc)) for acc in history.history['accuracy']],\n        'val_loss': [float(np.float32(val_loss)) for val_loss in history.history['val_loss']],\n        'val_accuracy': [float(np.float32(val_acc)) for val_acc in history.history['val_accuracy']]\n    }\n\n    # Store model parameters and performance in a dictionary\n    model_result = {\n        'lstm_units': lstm_units,\n        'cnn_filters': cnn_filters,\n        'kernel_size': kernel_size,\n        'attention_heads': attention_heads,\n        'history': history_data\n    }\n\n    # Append to results list and immediately save it to the file\n    results.append(model_result)\n    with open(results_file, 'w') as f:\n        json.dump(results, f)\n\n    # Clear the model from memory\n    tf.keras.backend.clear_session()\n    gc.collect()  # Explicit garbage collection\n\n    # Print the progress to monitor during execution\n    print(f\"Completed training with LSTM units={lstm_units}, CNN filters={cnn_filters}, kernel size={kernel_size}, attention heads={attention_heads}\")\n\n# After all configurations have run, results will contain the complete list of experiments.\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T15:38:24.233239Z","iopub.execute_input":"2024-11-11T15:38:24.233630Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 43ms/step - accuracy: 0.1376 - loss: 11.1102 - val_accuracy: 0.4546 - val_loss: 3.2949\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.3759 - loss: 3.3066 - val_accuracy: 0.5358 - val_loss: 2.3942\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.4736 - loss: 2.5208 - val_accuracy: 0.5451 - val_loss: 2.1742\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.4775 - loss: 2.3339 - val_accuracy: 0.5676 - val_loss: 1.9104\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5313 - loss: 2.0475 - val_accuracy: 0.5846 - val_loss: 1.8073\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5389 - loss: 1.9873 - val_accuracy: 0.5679 - val_loss: 1.8710\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5392 - loss: 1.9967 - val_accuracy: 0.5714 - val_loss: 1.7860\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5652 - loss: 1.8451 - val_accuracy: 0.5878 - val_loss: 1.6987\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5976 - loss: 1.7405 - val_accuracy: 0.6069 - val_loss: 1.6389\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6137 - loss: 1.6659 - val_accuracy: 0.6170 - val_loss: 1.6090\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6268 - loss: 1.6285 - val_accuracy: 0.5875 - val_loss: 1.8258\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5754 - loss: 1.8371 - val_accuracy: 0.5902 - val_loss: 1.7189\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5932 - loss: 1.7546 - val_accuracy: 0.5788 - val_loss: 1.7309\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6042 - loss: 1.7338 - val_accuracy: 0.6053 - val_loss: 1.6304\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6058 - loss: 1.6846 - val_accuracy: 0.6141 - val_loss: 1.6027\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6241 - loss: 1.6036 - val_accuracy: 0.6212 - val_loss: 1.5581\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6413 - loss: 1.5483 - val_accuracy: 0.6064 - val_loss: 1.6200\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.6533 - loss: 1.5030 - val_accuracy: 0.6332 - val_loss: 1.5422\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6636 - loss: 1.4813 - val_accuracy: 0.6334 - val_loss: 1.5320\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6716 - loss: 1.4276 - val_accuracy: 0.6345 - val_loss: 1.5403\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6908 - loss: 1.3872 - val_accuracy: 0.6382 - val_loss: 1.5303\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6845 - loss: 1.4074 - val_accuracy: 0.5910 - val_loss: 1.7792\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6223 - loss: 1.6902 - val_accuracy: 0.6170 - val_loss: 1.6321\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6292 - loss: 1.6202 - val_accuracy: 0.6146 - val_loss: 1.6124\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6384 - loss: 1.5880 - val_accuracy: 0.6199 - val_loss: 1.5959\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6347 - loss: 1.5925 - val_accuracy: 0.6042 - val_loss: 1.6357\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6463 - loss: 1.5564 - val_accuracy: 0.6172 - val_loss: 1.5888\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6415 - loss: 1.5444 - val_accuracy: 0.6172 - val_loss: 1.5927\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6503 - loss: 1.5197 - val_accuracy: 0.6347 - val_loss: 1.5467\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6670 - loss: 1.4738 - val_accuracy: 0.6284 - val_loss: 1.5615\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6670 - loss: 1.4693 - val_accuracy: 0.6308 - val_loss: 1.5666\nCompleted training with LSTM units=128, CNN filters=64, kernel size=3, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - accuracy: 0.1408 - loss: 10.9271 - val_accuracy: 0.4881 - val_loss: 3.1732\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.4168 - loss: 3.2131 - val_accuracy: 0.5446 - val_loss: 2.3413\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.4854 - loss: 2.4788 - val_accuracy: 0.5271 - val_loss: 2.2537\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.4837 - loss: 2.3244 - val_accuracy: 0.5613 - val_loss: 1.9330\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5216 - loss: 2.0488 - val_accuracy: 0.5875 - val_loss: 1.8281\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5449 - loss: 1.9419 - val_accuracy: 0.5432 - val_loss: 1.9747\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5280 - loss: 2.0072 - val_accuracy: 0.5780 - val_loss: 1.7602\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5532 - loss: 1.8815 - val_accuracy: 0.5700 - val_loss: 1.7763\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5825 - loss: 1.7677 - val_accuracy: 0.6066 - val_loss: 1.6523\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6030 - loss: 1.6696 - val_accuracy: 0.6082 - val_loss: 1.6266\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6141 - loss: 1.6349 - val_accuracy: 0.5825 - val_loss: 1.7967\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5785 - loss: 1.8429 - val_accuracy: 0.5859 - val_loss: 1.7674\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5829 - loss: 1.7760 - val_accuracy: 0.6053 - val_loss: 1.6795\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6006 - loss: 1.7183 - val_accuracy: 0.6056 - val_loss: 1.6464\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6129 - loss: 1.6566 - val_accuracy: 0.5973 - val_loss: 1.6448\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6248 - loss: 1.6320 - val_accuracy: 0.6029 - val_loss: 1.6297\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6273 - loss: 1.5698 - val_accuracy: 0.6233 - val_loss: 1.5644\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6541 - loss: 1.5141 - val_accuracy: 0.6281 - val_loss: 1.5569\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6605 - loss: 1.4720 - val_accuracy: 0.6321 - val_loss: 1.5362\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6692 - loss: 1.4443 - val_accuracy: 0.6308 - val_loss: 1.5422\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6836 - loss: 1.3974 - val_accuracy: 0.6310 - val_loss: 1.5338\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6788 - loss: 1.4142 - val_accuracy: 0.6178 - val_loss: 1.7127\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6172 - loss: 1.7132 - val_accuracy: 0.6130 - val_loss: 1.6484\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6354 - loss: 1.6365 - val_accuracy: 0.6077 - val_loss: 1.6629\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6327 - loss: 1.6263 - val_accuracy: 0.6143 - val_loss: 1.6206\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6308 - loss: 1.6041 - val_accuracy: 0.6215 - val_loss: 1.6246\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6432 - loss: 1.5642 - val_accuracy: 0.6196 - val_loss: 1.5978\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6516 - loss: 1.5455 - val_accuracy: 0.6175 - val_loss: 1.6068\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6460 - loss: 1.5361 - val_accuracy: 0.6194 - val_loss: 1.6054\nCompleted training with LSTM units=128, CNN filters=64, kernel size=3, attention heads=16\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 43ms/step - accuracy: 0.1367 - loss: 10.8612 - val_accuracy: 0.4631 - val_loss: 3.1103\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.3938 - loss: 3.1710 - val_accuracy: 0.5549 - val_loss: 2.3416\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.4846 - loss: 2.4867 - val_accuracy: 0.5167 - val_loss: 2.2553\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5043 - loss: 2.3004 - val_accuracy: 0.5775 - val_loss: 1.9493\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5506 - loss: 2.0285 - val_accuracy: 0.6164 - val_loss: 1.7940\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5720 - loss: 1.9407 - val_accuracy: 0.6019 - val_loss: 1.8477\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5533 - loss: 1.9688 - val_accuracy: 0.5934 - val_loss: 1.7843\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5744 - loss: 1.8614 - val_accuracy: 0.6093 - val_loss: 1.7087\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6035 - loss: 1.7432 - val_accuracy: 0.6302 - val_loss: 1.6229\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6275 - loss: 1.6572 - val_accuracy: 0.6337 - val_loss: 1.6020\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6355 - loss: 1.6213 - val_accuracy: 0.5674 - val_loss: 1.8841\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5835 - loss: 1.8748 - val_accuracy: 0.6074 - val_loss: 1.7170\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5895 - loss: 1.7997 - val_accuracy: 0.6130 - val_loss: 1.6675\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6175 - loss: 1.6946 - val_accuracy: 0.5984 - val_loss: 1.6920\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6239 - loss: 1.6578 - val_accuracy: 0.6223 - val_loss: 1.6186\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6313 - loss: 1.6116 - val_accuracy: 0.6249 - val_loss: 1.5890\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6480 - loss: 1.5240 - val_accuracy: 0.6273 - val_loss: 1.5712\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6685 - loss: 1.4878 - val_accuracy: 0.6345 - val_loss: 1.5539\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6784 - loss: 1.4398 - val_accuracy: 0.6438 - val_loss: 1.5470\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6805 - loss: 1.4080 - val_accuracy: 0.6390 - val_loss: 1.5381\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6907 - loss: 1.3948 - val_accuracy: 0.6414 - val_loss: 1.5414\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6872 - loss: 1.3986 - val_accuracy: 0.6239 - val_loss: 1.7067\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6263 - loss: 1.6865 - val_accuracy: 0.5926 - val_loss: 1.6909\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6249 - loss: 1.6640 - val_accuracy: 0.6204 - val_loss: 1.6731\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6469 - loss: 1.5790 - val_accuracy: 0.6263 - val_loss: 1.6176\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6515 - loss: 1.5622 - val_accuracy: 0.6302 - val_loss: 1.6186\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6503 - loss: 1.5611 - val_accuracy: 0.6151 - val_loss: 1.6369\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6535 - loss: 1.5251 - val_accuracy: 0.6146 - val_loss: 1.6279\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6638 - loss: 1.4920 - val_accuracy: 0.6202 - val_loss: 1.6027\nCompleted training with LSTM units=128, CNN filters=64, kernel size=5, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 47ms/step - accuracy: 0.1363 - loss: 10.7256 - val_accuracy: 0.4724 - val_loss: 3.0908\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.3693 - loss: 3.1401 - val_accuracy: 0.5050 - val_loss: 2.3086\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.4766 - loss: 2.4075 - val_accuracy: 0.5080 - val_loss: 2.1740\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.4789 - loss: 2.2673 - val_accuracy: 0.5451 - val_loss: 1.9341\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.5167 - loss: 2.0181 - val_accuracy: 0.5767 - val_loss: 1.8148\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5428 - loss: 1.9372 - val_accuracy: 0.5504 - val_loss: 1.9033\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.5272 - loss: 1.9896 - val_accuracy: 0.5655 - val_loss: 1.7926\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5484 - loss: 1.8678 - val_accuracy: 0.5748 - val_loss: 1.7188\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.5732 - loss: 1.7526 - val_accuracy: 0.5915 - val_loss: 1.6721\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5929 - loss: 1.6917 - val_accuracy: 0.6037 - val_loss: 1.6335\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6089 - loss: 1.6181 - val_accuracy: 0.5658 - val_loss: 1.8125\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5555 - loss: 1.8813 - val_accuracy: 0.5684 - val_loss: 1.7727\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5799 - loss: 1.7880 - val_accuracy: 0.5785 - val_loss: 1.7188\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.5883 - loss: 1.7270 - val_accuracy: 0.5928 - val_loss: 1.6454\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6005 - loss: 1.6720 - val_accuracy: 0.5881 - val_loss: 1.6549\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6065 - loss: 1.6398 - val_accuracy: 0.5963 - val_loss: 1.6285\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6262 - loss: 1.5689 - val_accuracy: 0.6138 - val_loss: 1.5860\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6425 - loss: 1.4981 - val_accuracy: 0.6114 - val_loss: 1.5705\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6560 - loss: 1.4773 - val_accuracy: 0.6164 - val_loss: 1.5533\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6599 - loss: 1.4300 - val_accuracy: 0.6202 - val_loss: 1.5491\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6697 - loss: 1.4050 - val_accuracy: 0.6231 - val_loss: 1.5405\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6643 - loss: 1.4404 - val_accuracy: 0.5745 - val_loss: 1.8285\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6076 - loss: 1.7115 - val_accuracy: 0.5958 - val_loss: 1.6753\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6117 - loss: 1.6547 - val_accuracy: 0.6008 - val_loss: 1.6270\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6120 - loss: 1.6352 - val_accuracy: 0.5926 - val_loss: 1.6726\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6218 - loss: 1.5986 - val_accuracy: 0.5851 - val_loss: 1.6536\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6450 - loss: 1.5397 - val_accuracy: 0.6119 - val_loss: 1.5993\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6450 - loss: 1.5409 - val_accuracy: 0.5928 - val_loss: 1.6656\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6316 - loss: 1.5546 - val_accuracy: 0.6053 - val_loss: 1.6068\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6519 - loss: 1.5156 - val_accuracy: 0.6095 - val_loss: 1.5663\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6625 - loss: 1.4575 - val_accuracy: 0.6241 - val_loss: 1.5596\nEpoch 32/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6569 - loss: 1.4411 - val_accuracy: 0.6175 - val_loss: 1.5561\nEpoch 33/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6769 - loss: 1.3902 - val_accuracy: 0.6324 - val_loss: 1.5202\nEpoch 34/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6800 - loss: 1.3740 - val_accuracy: 0.6257 - val_loss: 1.5413\nEpoch 35/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6910 - loss: 1.3357 - val_accuracy: 0.6345 - val_loss: 1.5231\nEpoch 36/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7008 - loss: 1.3084 - val_accuracy: 0.6334 - val_loss: 1.5354\nEpoch 37/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7005 - loss: 1.2966 - val_accuracy: 0.6342 - val_loss: 1.5430\nEpoch 38/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7128 - loss: 1.2335 - val_accuracy: 0.6361 - val_loss: 1.5260\nEpoch 39/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.7218 - loss: 1.2259 - val_accuracy: 0.6438 - val_loss: 1.5448\nEpoch 40/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7305 - loss: 1.1964 - val_accuracy: 0.6401 - val_loss: 1.5470\nEpoch 41/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7321 - loss: 1.1891 - val_accuracy: 0.6435 - val_loss: 1.5417\nEpoch 42/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7358 - loss: 1.1892 - val_accuracy: 0.6469 - val_loss: 1.5455\nEpoch 43/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.7360 - loss: 1.1850 - val_accuracy: 0.6451 - val_loss: 1.5463\nEpoch 44/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6990 - loss: 1.3547 - val_accuracy: 0.6029 - val_loss: 1.6600\nEpoch 45/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6389 - loss: 1.5707 - val_accuracy: 0.6241 - val_loss: 1.5691\nEpoch 46/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6626 - loss: 1.4694 - val_accuracy: 0.6164 - val_loss: 1.5933\nEpoch 47/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6630 - loss: 1.4900 - val_accuracy: 0.6276 - val_loss: 1.5696\nEpoch 48/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6683 - loss: 1.4530 - val_accuracy: 0.6249 - val_loss: 1.5791\nEpoch 49/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6706 - loss: 1.4362 - val_accuracy: 0.6159 - val_loss: 1.5967\nEpoch 50/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6674 - loss: 1.4689 - val_accuracy: 0.6188 - val_loss: 1.5982\nEpoch 51/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6741 - loss: 1.4323 - val_accuracy: 0.6194 - val_loss: 1.6384\nEpoch 52/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6848 - loss: 1.4223 - val_accuracy: 0.6260 - val_loss: 1.5623\nCompleted training with LSTM units=128, CNN filters=64, kernel size=5, attention heads=16\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 43ms/step - accuracy: 0.1438 - loss: 11.6963 - val_accuracy: 0.4928 - val_loss: 3.3312\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.4339 - loss: 3.3420 - val_accuracy: 0.5623 - val_loss: 2.3637\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5151 - loss: 2.4788 - val_accuracy: 0.5257 - val_loss: 2.3179\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5110 - loss: 2.3034 - val_accuracy: 0.5841 - val_loss: 1.9129\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5695 - loss: 1.9870 - val_accuracy: 0.6114 - val_loss: 1.7796\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5718 - loss: 1.9296 - val_accuracy: 0.5918 - val_loss: 1.8794\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5557 - loss: 1.9914 - val_accuracy: 0.5772 - val_loss: 1.8252\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5815 - loss: 1.8471 - val_accuracy: 0.6159 - val_loss: 1.6794\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6035 - loss: 1.7360 - val_accuracy: 0.6172 - val_loss: 1.6251\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6300 - loss: 1.6308 - val_accuracy: 0.6342 - val_loss: 1.5842\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6497 - loss: 1.5694 - val_accuracy: 0.5748 - val_loss: 1.8849\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.5817 - loss: 1.9288 - val_accuracy: 0.5955 - val_loss: 1.7239\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6086 - loss: 1.7534 - val_accuracy: 0.6093 - val_loss: 1.6746\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6099 - loss: 1.7265 - val_accuracy: 0.6090 - val_loss: 1.6756\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6294 - loss: 1.6537 - val_accuracy: 0.6252 - val_loss: 1.5929\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6474 - loss: 1.5886 - val_accuracy: 0.6377 - val_loss: 1.5760\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6531 - loss: 1.5334 - val_accuracy: 0.6355 - val_loss: 1.5382\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6721 - loss: 1.4645 - val_accuracy: 0.6355 - val_loss: 1.5303\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6775 - loss: 1.4309 - val_accuracy: 0.6408 - val_loss: 1.5228\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6937 - loss: 1.3551 - val_accuracy: 0.6467 - val_loss: 1.5009\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.7034 - loss: 1.3546 - val_accuracy: 0.6504 - val_loss: 1.5009\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.7056 - loss: 1.3680 - val_accuracy: 0.6045 - val_loss: 1.8046\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6370 - loss: 1.6679 - val_accuracy: 0.6072 - val_loss: 1.6535\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6397 - loss: 1.6107 - val_accuracy: 0.6133 - val_loss: 1.6308\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6468 - loss: 1.5794 - val_accuracy: 0.6281 - val_loss: 1.5715\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - accuracy: 0.6569 - loss: 1.5668 - val_accuracy: 0.6305 - val_loss: 1.5534\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6502 - loss: 1.5414 - val_accuracy: 0.6053 - val_loss: 1.6276\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6703 - loss: 1.5087 - val_accuracy: 0.6387 - val_loss: 1.5408\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6639 - loss: 1.4919 - val_accuracy: 0.6064 - val_loss: 1.6172\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6794 - loss: 1.4525 - val_accuracy: 0.6419 - val_loss: 1.5161\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.6832 - loss: 1.4298 - val_accuracy: 0.6475 - val_loss: 1.5048\nCompleted training with LSTM units=128, CNN filters=128, kernel size=3, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 47ms/step - accuracy: 0.1505 - loss: 11.5975 - val_accuracy: 0.4732 - val_loss: 3.3005\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.4144 - loss: 3.3067 - val_accuracy: 0.5318 - val_loss: 2.3644\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.4909 - loss: 2.4788 - val_accuracy: 0.4944 - val_loss: 2.2547\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.4962 - loss: 2.2989 - val_accuracy: 0.5862 - val_loss: 1.8869\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5467 - loss: 2.0111 - val_accuracy: 0.6053 - val_loss: 1.7578\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5645 - loss: 1.9110 - val_accuracy: 0.5719 - val_loss: 1.8515\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5408 - loss: 1.9898 - val_accuracy: 0.5952 - val_loss: 1.7607\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5729 - loss: 1.8486 - val_accuracy: 0.6210 - val_loss: 1.6506\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6046 - loss: 1.7246 - val_accuracy: 0.6175 - val_loss: 1.6035\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6276 - loss: 1.6159 - val_accuracy: 0.6263 - val_loss: 1.5718\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6338 - loss: 1.5531 - val_accuracy: 0.5695 - val_loss: 1.8070\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5777 - loss: 1.8289 - val_accuracy: 0.5857 - val_loss: 1.7241\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5977 - loss: 1.7667 - val_accuracy: 0.6056 - val_loss: 1.6221\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6144 - loss: 1.6665 - val_accuracy: 0.6223 - val_loss: 1.5969\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6252 - loss: 1.6235 - val_accuracy: 0.6141 - val_loss: 1.6250\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6325 - loss: 1.5728 - val_accuracy: 0.6279 - val_loss: 1.5532\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6492 - loss: 1.5259 - val_accuracy: 0.6167 - val_loss: 1.5577\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6643 - loss: 1.4592 - val_accuracy: 0.6462 - val_loss: 1.5182\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6863 - loss: 1.4015 - val_accuracy: 0.6406 - val_loss: 1.5083\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6972 - loss: 1.3534 - val_accuracy: 0.6435 - val_loss: 1.5018\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7062 - loss: 1.3174 - val_accuracy: 0.6469 - val_loss: 1.4994\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7069 - loss: 1.3415 - val_accuracy: 0.6024 - val_loss: 1.7541\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6289 - loss: 1.6657 - val_accuracy: 0.6146 - val_loss: 1.6581\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6355 - loss: 1.6127 - val_accuracy: 0.6260 - val_loss: 1.5783\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6422 - loss: 1.5640 - val_accuracy: 0.6188 - val_loss: 1.5554\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6473 - loss: 1.5612 - val_accuracy: 0.6345 - val_loss: 1.5618\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6606 - loss: 1.5243 - val_accuracy: 0.6310 - val_loss: 1.5716\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6578 - loss: 1.4909 - val_accuracy: 0.6260 - val_loss: 1.5809\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6718 - loss: 1.4767 - val_accuracy: 0.6403 - val_loss: 1.5181\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6770 - loss: 1.4432 - val_accuracy: 0.6358 - val_loss: 1.5552\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6807 - loss: 1.4270 - val_accuracy: 0.6477 - val_loss: 1.5009\nEpoch 32/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6892 - loss: 1.3828 - val_accuracy: 0.6284 - val_loss: 1.5470\nEpoch 33/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6902 - loss: 1.3842 - val_accuracy: 0.6430 - val_loss: 1.5228\nEpoch 34/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7058 - loss: 1.3294 - val_accuracy: 0.6408 - val_loss: 1.5029\nEpoch 35/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7161 - loss: 1.2909 - val_accuracy: 0.6446 - val_loss: 1.4922\nEpoch 36/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7201 - loss: 1.2694 - val_accuracy: 0.6480 - val_loss: 1.5003\nEpoch 37/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7231 - loss: 1.2467 - val_accuracy: 0.6477 - val_loss: 1.4952\nEpoch 38/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7413 - loss: 1.1848 - val_accuracy: 0.6509 - val_loss: 1.4898\nEpoch 39/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7419 - loss: 1.1712 - val_accuracy: 0.6480 - val_loss: 1.4989\nEpoch 40/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7537 - loss: 1.1569 - val_accuracy: 0.6523 - val_loss: 1.5078\nEpoch 41/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7594 - loss: 1.1260 - val_accuracy: 0.6546 - val_loss: 1.5017\nEpoch 42/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7608 - loss: 1.1145 - val_accuracy: 0.6554 - val_loss: 1.5058\nEpoch 43/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7557 - loss: 1.1326 - val_accuracy: 0.6578 - val_loss: 1.5100\nEpoch 44/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7391 - loss: 1.2649 - val_accuracy: 0.6276 - val_loss: 1.5878\nEpoch 45/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6757 - loss: 1.4683 - val_accuracy: 0.6302 - val_loss: 1.5815\nEpoch 46/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6802 - loss: 1.4393 - val_accuracy: 0.6236 - val_loss: 1.5833\nEpoch 47/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6823 - loss: 1.4381 - val_accuracy: 0.6414 - val_loss: 1.5618\nEpoch 48/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6777 - loss: 1.4621 - val_accuracy: 0.6371 - val_loss: 1.5523\nEpoch 49/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6818 - loss: 1.4385 - val_accuracy: 0.6477 - val_loss: 1.5346\nEpoch 50/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6812 - loss: 1.4446 - val_accuracy: 0.6408 - val_loss: 1.5448\nEpoch 51/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6900 - loss: 1.4088 - val_accuracy: 0.6401 - val_loss: 1.5367\nEpoch 52/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6852 - loss: 1.4295 - val_accuracy: 0.6387 - val_loss: 1.5407\nEpoch 53/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6907 - loss: 1.4195 - val_accuracy: 0.6509 - val_loss: 1.5061\nCompleted training with LSTM units=128, CNN filters=128, kernel size=3, attention heads=16\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.1371 - loss: 11.5191 - val_accuracy: 0.5218 - val_loss: 3.1089\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.4210 - loss: 3.2038 - val_accuracy: 0.5565 - val_loss: 2.2988\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5055 - loss: 2.4207 - val_accuracy: 0.5393 - val_loss: 2.1847\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.4951 - loss: 2.2803 - val_accuracy: 0.5748 - val_loss: 1.8951\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5422 - loss: 2.0145 - val_accuracy: 0.6032 - val_loss: 1.7667\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5761 - loss: 1.9041 - val_accuracy: 0.5687 - val_loss: 1.8922\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5468 - loss: 1.9778 - val_accuracy: 0.5923 - val_loss: 1.7574\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5789 - loss: 1.8141 - val_accuracy: 0.6024 - val_loss: 1.6746\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5968 - loss: 1.7223 - val_accuracy: 0.6072 - val_loss: 1.6356\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6273 - loss: 1.6300 - val_accuracy: 0.6244 - val_loss: 1.5945\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6374 - loss: 1.5757 - val_accuracy: 0.5660 - val_loss: 1.8627\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5816 - loss: 1.8614 - val_accuracy: 0.6101 - val_loss: 1.6779\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6007 - loss: 1.7630 - val_accuracy: 0.5939 - val_loss: 1.7105\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.5952 - loss: 1.7883 - val_accuracy: 0.6090 - val_loss: 1.6318\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6227 - loss: 1.6619 - val_accuracy: 0.6093 - val_loss: 1.6329\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.6336 - loss: 1.6027 - val_accuracy: 0.6159 - val_loss: 1.5971\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6548 - loss: 1.5288 - val_accuracy: 0.6464 - val_loss: 1.5276\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6678 - loss: 1.4643 - val_accuracy: 0.6411 - val_loss: 1.5251\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6816 - loss: 1.4252 - val_accuracy: 0.6366 - val_loss: 1.5136\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6956 - loss: 1.3666 - val_accuracy: 0.6411 - val_loss: 1.5085\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6981 - loss: 1.3367 - val_accuracy: 0.6406 - val_loss: 1.5099\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7006 - loss: 1.3514 - val_accuracy: 0.5472 - val_loss: 1.9487\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6226 - loss: 1.7216 - val_accuracy: 0.6162 - val_loss: 1.6393\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6488 - loss: 1.6030 - val_accuracy: 0.6294 - val_loss: 1.6192\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6627 - loss: 1.5484 - val_accuracy: 0.6281 - val_loss: 1.5818\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6531 - loss: 1.5497 - val_accuracy: 0.6313 - val_loss: 1.5855\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.6546 - loss: 1.5453 - val_accuracy: 0.6191 - val_loss: 1.6103\nCompleted training with LSTM units=128, CNN filters=128, kernel size=5, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 48ms/step - accuracy: 0.1359 - loss: 11.4291 - val_accuracy: 0.5056 - val_loss: 3.0539\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.4230 - loss: 3.1411 - val_accuracy: 0.5475 - val_loss: 2.2775\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5010 - loss: 2.3936 - val_accuracy: 0.5366 - val_loss: 2.1436\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5009 - loss: 2.2826 - val_accuracy: 0.5812 - val_loss: 1.8866\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5561 - loss: 1.9587 - val_accuracy: 0.6019 - val_loss: 1.7628\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5621 - loss: 1.9123 - val_accuracy: 0.5719 - val_loss: 1.8482\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5352 - loss: 1.9548 - val_accuracy: 0.5926 - val_loss: 1.7431\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5721 - loss: 1.8134 - val_accuracy: 0.5891 - val_loss: 1.7004\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5912 - loss: 1.7209 - val_accuracy: 0.6019 - val_loss: 1.6108\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6266 - loss: 1.5977 - val_accuracy: 0.6188 - val_loss: 1.5878\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6397 - loss: 1.5521 - val_accuracy: 0.5838 - val_loss: 1.8498\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5798 - loss: 1.8601 - val_accuracy: 0.5836 - val_loss: 1.7399\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.5948 - loss: 1.7432 - val_accuracy: 0.5785 - val_loss: 1.7424\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6090 - loss: 1.6980 - val_accuracy: 0.6056 - val_loss: 1.6473\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6286 - loss: 1.6507 - val_accuracy: 0.6141 - val_loss: 1.6156\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6270 - loss: 1.6066 - val_accuracy: 0.6236 - val_loss: 1.5762\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6460 - loss: 1.5263 - val_accuracy: 0.6260 - val_loss: 1.5678\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6589 - loss: 1.4843 - val_accuracy: 0.6340 - val_loss: 1.5378\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6777 - loss: 1.4228 - val_accuracy: 0.6448 - val_loss: 1.5176\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6849 - loss: 1.3771 - val_accuracy: 0.6401 - val_loss: 1.5191\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6929 - loss: 1.3591 - val_accuracy: 0.6411 - val_loss: 1.5116\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6906 - loss: 1.3845 - val_accuracy: 0.5971 - val_loss: 1.7900\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6355 - loss: 1.6780 - val_accuracy: 0.6196 - val_loss: 1.6292\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6438 - loss: 1.5984 - val_accuracy: 0.6191 - val_loss: 1.6392\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6534 - loss: 1.5702 - val_accuracy: 0.6310 - val_loss: 1.6081\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6411 - loss: 1.5922 - val_accuracy: 0.6324 - val_loss: 1.5901\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6559 - loss: 1.5309 - val_accuracy: 0.6220 - val_loss: 1.5905\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6496 - loss: 1.5413 - val_accuracy: 0.6340 - val_loss: 1.5558\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.6625 - loss: 1.5238 - val_accuracy: 0.6401 - val_loss: 1.5388\nCompleted training with LSTM units=128, CNN filters=128, kernel size=5, attention heads=16\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 45ms/step - accuracy: 0.1438 - loss: 12.4613 - val_accuracy: 0.4984 - val_loss: 3.3699\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.4292 - loss: 3.3727 - val_accuracy: 0.5645 - val_loss: 2.3222\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5181 - loss: 2.4724 - val_accuracy: 0.5695 - val_loss: 2.1336\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5200 - loss: 2.2951 - val_accuracy: 0.5817 - val_loss: 1.8842\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5747 - loss: 1.9583 - val_accuracy: 0.6292 - val_loss: 1.7190\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5891 - loss: 1.8708 - val_accuracy: 0.5793 - val_loss: 1.8643\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5615 - loss: 1.9559 - val_accuracy: 0.5936 - val_loss: 1.7749\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5969 - loss: 1.8023 - val_accuracy: 0.6143 - val_loss: 1.6771\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6258 - loss: 1.6750 - val_accuracy: 0.6393 - val_loss: 1.5626\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6424 - loss: 1.5714 - val_accuracy: 0.6456 - val_loss: 1.5401\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6584 - loss: 1.5246 - val_accuracy: 0.5759 - val_loss: 1.8392\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.5897 - loss: 1.8779 - val_accuracy: 0.6204 - val_loss: 1.6750\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5987 - loss: 1.7798 - val_accuracy: 0.6228 - val_loss: 1.6062\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6220 - loss: 1.6683 - val_accuracy: 0.6204 - val_loss: 1.6125\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6336 - loss: 1.6236 - val_accuracy: 0.6284 - val_loss: 1.5733\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6358 - loss: 1.5829 - val_accuracy: 0.6403 - val_loss: 1.5059\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6482 - loss: 1.5117 - val_accuracy: 0.6363 - val_loss: 1.5157\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6759 - loss: 1.4432 - val_accuracy: 0.6390 - val_loss: 1.5031\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6911 - loss: 1.3826 - val_accuracy: 0.6422 - val_loss: 1.4862\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6987 - loss: 1.3614 - val_accuracy: 0.6488 - val_loss: 1.4696\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.7113 - loss: 1.2968 - val_accuracy: 0.6549 - val_loss: 1.4643\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.7055 - loss: 1.3329 - val_accuracy: 0.6040 - val_loss: 1.7373\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6431 - loss: 1.6452 - val_accuracy: 0.6454 - val_loss: 1.5800\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6474 - loss: 1.6014 - val_accuracy: 0.6393 - val_loss: 1.5526\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6499 - loss: 1.5560 - val_accuracy: 0.6411 - val_loss: 1.5786\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6577 - loss: 1.5588 - val_accuracy: 0.6297 - val_loss: 1.5637\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6663 - loss: 1.5017 - val_accuracy: 0.6493 - val_loss: 1.5501\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6679 - loss: 1.4942 - val_accuracy: 0.6440 - val_loss: 1.5327\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6783 - loss: 1.4758 - val_accuracy: 0.6347 - val_loss: 1.5285\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6760 - loss: 1.4254 - val_accuracy: 0.6342 - val_loss: 1.4989\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6832 - loss: 1.3989 - val_accuracy: 0.6451 - val_loss: 1.5066\nCompleted training with LSTM units=128, CNN filters=256, kernel size=3, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 50ms/step - accuracy: 0.0895 - loss: 11.8422 - val_accuracy: 0.4082 - val_loss: 2.7759\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.3422 - loss: 2.8866 - val_accuracy: 0.5066 - val_loss: 2.0891\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.4667 - loss: 2.2118 - val_accuracy: 0.5191 - val_loss: 1.9870\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.4652 - loss: 2.1527 - val_accuracy: 0.5393 - val_loss: 1.7990\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5159 - loss: 1.8884 - val_accuracy: 0.5695 - val_loss: 1.6704\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5368 - loss: 1.8160 - val_accuracy: 0.5493 - val_loss: 1.8365\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5262 - loss: 1.8943 - val_accuracy: 0.5467 - val_loss: 1.7560\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5428 - loss: 1.7745 - val_accuracy: 0.5798 - val_loss: 1.6291\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5752 - loss: 1.6672 - val_accuracy: 0.5968 - val_loss: 1.5786\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5924 - loss: 1.5783 - val_accuracy: 0.6019 - val_loss: 1.5444\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5976 - loss: 1.5566 - val_accuracy: 0.5658 - val_loss: 1.7558\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.5565 - loss: 1.7808 - val_accuracy: 0.5477 - val_loss: 1.7381\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5695 - loss: 1.7188 - val_accuracy: 0.5761 - val_loss: 1.6768\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5822 - loss: 1.6794 - val_accuracy: 0.5973 - val_loss: 1.5853\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5941 - loss: 1.6327 - val_accuracy: 0.6045 - val_loss: 1.5658\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6173 - loss: 1.5507 - val_accuracy: 0.6021 - val_loss: 1.5674\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6285 - loss: 1.5034 - val_accuracy: 0.6175 - val_loss: 1.5049\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6416 - loss: 1.4590 - val_accuracy: 0.6151 - val_loss: 1.5048\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6518 - loss: 1.4122 - val_accuracy: 0.6265 - val_loss: 1.4790\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6641 - loss: 1.3688 - val_accuracy: 0.6316 - val_loss: 1.4638\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6764 - loss: 1.3273 - val_accuracy: 0.6294 - val_loss: 1.4668\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6671 - loss: 1.3660 - val_accuracy: 0.5727 - val_loss: 1.7381\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6116 - loss: 1.6526 - val_accuracy: 0.6186 - val_loss: 1.5541\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6213 - loss: 1.5752 - val_accuracy: 0.6074 - val_loss: 1.6190\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.6285 - loss: 1.5664 - val_accuracy: 0.6135 - val_loss: 1.5340\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6293 - loss: 1.5267 - val_accuracy: 0.6196 - val_loss: 1.5425\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6314 - loss: 1.5184 - val_accuracy: 0.5934 - val_loss: 1.6160\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6493 - loss: 1.4830 - val_accuracy: 0.6082 - val_loss: 1.5524\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6563 - loss: 1.4400 - val_accuracy: 0.6239 - val_loss: 1.4999\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6616 - loss: 1.4257 - val_accuracy: 0.6305 - val_loss: 1.5013\nCompleted training with LSTM units=128, CNN filters=256, kernel size=3, attention heads=16\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - accuracy: 0.1182 - loss: 12.2110 - val_accuracy: 0.4708 - val_loss: 3.0717\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.3998 - loss: 3.1264 - val_accuracy: 0.5398 - val_loss: 2.2395\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.4903 - loss: 2.3365 - val_accuracy: 0.5499 - val_loss: 2.0707\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.4788 - loss: 2.2417 - val_accuracy: 0.5552 - val_loss: 1.8744\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5465 - loss: 1.9074 - val_accuracy: 0.5931 - val_loss: 1.7185\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5605 - loss: 1.8536 - val_accuracy: 0.5597 - val_loss: 1.8580\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5319 - loss: 1.9396 - val_accuracy: 0.5801 - val_loss: 1.7052\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5669 - loss: 1.7915 - val_accuracy: 0.5812 - val_loss: 1.6580\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5835 - loss: 1.7033 - val_accuracy: 0.6080 - val_loss: 1.5734\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6246 - loss: 1.5673 - val_accuracy: 0.6146 - val_loss: 1.5515\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6386 - loss: 1.5155 - val_accuracy: 0.5690 - val_loss: 1.8376\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.5789 - loss: 1.8479 - val_accuracy: 0.5711 - val_loss: 1.7602\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5874 - loss: 1.7195 - val_accuracy: 0.5809 - val_loss: 1.6836\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6035 - loss: 1.6637 - val_accuracy: 0.5828 - val_loss: 1.6387\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6052 - loss: 1.6344 - val_accuracy: 0.6141 - val_loss: 1.5562\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6245 - loss: 1.5606 - val_accuracy: 0.6212 - val_loss: 1.5550\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6385 - loss: 1.5046 - val_accuracy: 0.6220 - val_loss: 1.5419\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6543 - loss: 1.4490 - val_accuracy: 0.6276 - val_loss: 1.5083\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6676 - loss: 1.3874 - val_accuracy: 0.6379 - val_loss: 1.5013\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6730 - loss: 1.3500 - val_accuracy: 0.6374 - val_loss: 1.4891\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6870 - loss: 1.3271 - val_accuracy: 0.6422 - val_loss: 1.4852\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6907 - loss: 1.3335 - val_accuracy: 0.5756 - val_loss: 1.8100\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6136 - loss: 1.6821 - val_accuracy: 0.5926 - val_loss: 1.6474\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6124 - loss: 1.5951 - val_accuracy: 0.6074 - val_loss: 1.6099\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6338 - loss: 1.5569 - val_accuracy: 0.6000 - val_loss: 1.6504\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6366 - loss: 1.5479 - val_accuracy: 0.6159 - val_loss: 1.5552\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6490 - loss: 1.5189 - val_accuracy: 0.6305 - val_loss: 1.5187\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.6569 - loss: 1.4997 - val_accuracy: 0.6347 - val_loss: 1.5487\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6521 - loss: 1.4849 - val_accuracy: 0.6149 - val_loss: 1.5444\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6690 - loss: 1.4408 - val_accuracy: 0.6342 - val_loss: 1.5255\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.6742 - loss: 1.4218 - val_accuracy: 0.6268 - val_loss: 1.5709\nCompleted training with LSTM units=128, CNN filters=256, kernel size=5, attention heads=8\nEpoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 51ms/step - accuracy: 0.1344 - loss: 12.0031 - val_accuracy: 0.4663 - val_loss: 3.1175\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.4138 - loss: 3.1182 - val_accuracy: 0.5284 - val_loss: 2.2487\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.4898 - loss: 2.3541 - val_accuracy: 0.5138 - val_loss: 2.1791\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.4875 - loss: 2.2272 - val_accuracy: 0.5623 - val_loss: 1.8732\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5541 - loss: 1.8984 - val_accuracy: 0.5897 - val_loss: 1.7209\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5614 - loss: 1.8535 - val_accuracy: 0.5663 - val_loss: 1.8569\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5435 - loss: 1.9380 - val_accuracy: 0.5499 - val_loss: 1.7581\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5556 - loss: 1.8550 - val_accuracy: 0.5857 - val_loss: 1.6537\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5910 - loss: 1.6824 - val_accuracy: 0.5952 - val_loss: 1.6043\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6105 - loss: 1.6076 - val_accuracy: 0.6130 - val_loss: 1.5604\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6281 - loss: 1.5320 - val_accuracy: 0.5698 - val_loss: 1.7882\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5826 - loss: 1.8232 - val_accuracy: 0.5897 - val_loss: 1.6820\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5831 - loss: 1.7618 - val_accuracy: 0.5645 - val_loss: 1.7446\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6030 - loss: 1.6893 - val_accuracy: 0.5873 - val_loss: 1.6673\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6160 - loss: 1.6335 - val_accuracy: 0.6159 - val_loss: 1.5927\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6278 - loss: 1.5863 - val_accuracy: 0.6334 - val_loss: 1.5289\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6524 - loss: 1.4884 - val_accuracy: 0.6271 - val_loss: 1.5113\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6663 - loss: 1.4394 - val_accuracy: 0.6241 - val_loss: 1.5018\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6807 - loss: 1.3859 - val_accuracy: 0.6451 - val_loss: 1.4771\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6834 - loss: 1.3527 - val_accuracy: 0.6523 - val_loss: 1.4757\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7017 - loss: 1.3181 - val_accuracy: 0.6536 - val_loss: 1.4751\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.7092 - loss: 1.3088 - val_accuracy: 0.5939 - val_loss: 1.7527\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6290 - loss: 1.6658 - val_accuracy: 0.6202 - val_loss: 1.5952\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6395 - loss: 1.5876 - val_accuracy: 0.6215 - val_loss: 1.5846\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6444 - loss: 1.5480 - val_accuracy: 0.6241 - val_loss: 1.5857\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6444 - loss: 1.5475 - val_accuracy: 0.6135 - val_loss: 1.5975\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6542 - loss: 1.5100 - val_accuracy: 0.6183 - val_loss: 1.5796\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6670 - loss: 1.4961 - val_accuracy: 0.6215 - val_loss: 1.5930\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6672 - loss: 1.4540 - val_accuracy: 0.6355 - val_loss: 1.5352\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6727 - loss: 1.4738 - val_accuracy: 0.6350 - val_loss: 1.5204\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6851 - loss: 1.4085 - val_accuracy: 0.6321 - val_loss: 1.5445\nCompleted training with LSTM units=128, CNN filters=256, kernel size=5, attention heads=16\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Load results from file (assuming results were stored in 'model_results.json')\nresults_file = \"/kaggle/working/model_results.json\"\n\nwith open(results_file, 'r') as f:\n    results = json.load(f)\n\n# Find the best model based on highest validation accuracy\nbest_model = max(results, key=lambda x: max(x['history']['val_accuracy']))\n\n# Extract best model stats\nbest_stats = {\n    \"LSTM Units\": best_model['lstm_units'],\n    \"CNN Filters\": best_model['cnn_filters'],\n    \"Kernel Size\": best_model['kernel_size'],\n    \"Attention Heads\": best_model['attention_heads'],\n    \"Best Validation Accuracy\": max(best_model['history']['val_accuracy']),\n    \"Validation Loss at Best Accuracy\": best_model['history']['val_loss'][best_model['history']['val_accuracy'].index(max(best_model['history']['val_accuracy']))]\n}\n\nbest_stats\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM-CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MultiHeadAttention, Dense, Dropout, Bidirectional, LSTM, Input, LayerNormalization, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nfixed_lstm_units = 256  # Best LSTM units\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the LSTM-CNN model\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# LSTM layer with bidirectional configuration\nx = Bidirectional(LSTM(fixed_lstm_units, return_sequences=True, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))(inputs)\nx = Dropout(0.3)(x)  # Dropout after LSTM to prevent overfitting\n\n# CNN layer to capture spatial patterns from the LSTM output\nx = Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=l2(0.01))(x)\nx = Dropout(0.3)(x)  # Dropout after CNN\n\n# Multi-head attention layer with LayerNormalization\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(x, x)\nattention_output = LayerNormalization()(attention_output + x)\n\n# Global Average Pooling\npooled_output = GlobalAveragePooling1D()(attention_output)\n\n# Dense layers with L2 regularization and Dropout\nx = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(pooled_output)\nx = Dropout(0.3)(x)\nx = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)\nx = Dropout(0.3)(x)\noutputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,  # Limit epochs to 100 to reduce time and memory usage\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with LSTM-CNN model configuration\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T01:39:41.886736Z","iopub.execute_input":"2024-11-12T01:39:41.887124Z","iopub.status.idle":"2024-11-12T01:51:32.557002Z","shell.execute_reply.started":"2024-11-12T01:39:41.887093Z","shell.execute_reply":"2024-11-12T01:51:32.556106Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 54ms/step - accuracy: 0.1369 - loss: 13.6376 - val_accuracy: 0.4777 - val_loss: 2.9797\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4135 - loss: 3.0448 - val_accuracy: 0.5316 - val_loss: 2.2504\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4800 - loss: 2.3734 - val_accuracy: 0.5472 - val_loss: 2.1288\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.4852 - loss: 2.2898 - val_accuracy: 0.5695 - val_loss: 1.8949\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5448 - loss: 1.9684 - val_accuracy: 0.6040 - val_loss: 1.7494\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5585 - loss: 1.9007 - val_accuracy: 0.5668 - val_loss: 1.8668\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5395 - loss: 1.9951 - val_accuracy: 0.5854 - val_loss: 1.7698\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5692 - loss: 1.8508 - val_accuracy: 0.6003 - val_loss: 1.6905\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5974 - loss: 1.7237 - val_accuracy: 0.6133 - val_loss: 1.6038\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6228 - loss: 1.6383 - val_accuracy: 0.6257 - val_loss: 1.5748\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6231 - loss: 1.5876 - val_accuracy: 0.5777 - val_loss: 1.9424\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.5881 - loss: 1.9013 - val_accuracy: 0.5812 - val_loss: 1.7948\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.5950 - loss: 1.7945 - val_accuracy: 0.6101 - val_loss: 1.6681\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6107 - loss: 1.6922 - val_accuracy: 0.6215 - val_loss: 1.6134\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6129 - loss: 1.6829 - val_accuracy: 0.6098 - val_loss: 1.6187\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6290 - loss: 1.5980 - val_accuracy: 0.6212 - val_loss: 1.5914\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6525 - loss: 1.5312 - val_accuracy: 0.6321 - val_loss: 1.5327\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.6632 - loss: 1.4625 - val_accuracy: 0.6332 - val_loss: 1.5306\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6846 - loss: 1.4072 - val_accuracy: 0.6419 - val_loss: 1.5083\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6971 - loss: 1.3708 - val_accuracy: 0.6419 - val_loss: 1.4947\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6959 - loss: 1.3548 - val_accuracy: 0.6446 - val_loss: 1.4952\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6967 - loss: 1.3767 - val_accuracy: 0.6156 - val_loss: 1.7289\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6259 - loss: 1.6866 - val_accuracy: 0.6133 - val_loss: 1.6726\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6356 - loss: 1.6375 - val_accuracy: 0.6369 - val_loss: 1.5660\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6429 - loss: 1.5984 - val_accuracy: 0.6334 - val_loss: 1.5715\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6417 - loss: 1.5952 - val_accuracy: 0.6247 - val_loss: 1.6072\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6580 - loss: 1.5335 - val_accuracy: 0.6281 - val_loss: 1.5615\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6568 - loss: 1.5236 - val_accuracy: 0.6329 - val_loss: 1.5578\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6631 - loss: 1.5137 - val_accuracy: 0.6268 - val_loss: 1.5631\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6780 - loss: 1.4575 - val_accuracy: 0.6286 - val_loss: 1.5547\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.6871 - loss: 1.4215 - val_accuracy: 0.6377 - val_loss: 1.5337\nCompleted training with LSTM-CNN model configuration\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T01:51:32.559626Z","iopub.execute_input":"2024-11-12T01:51:32.560267Z","iopub.status.idle":"2024-11-12T01:51:38.285440Z","shell.execute_reply.started":"2024-11-12T01:51:32.560229Z","shell.execute_reply":"2024-11-12T01:51:38.284524Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6308 - loss: 1.5375\nTest Loss: 1.4952224493026733\nTest Accuracy: 0.6445623636245728\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CNN Attention","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Input, LayerNormalization, Concatenate, Flatten, Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for each Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the fixed best parameters\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# Multiple Conv1D layers with different kernel sizes for bi-gram, tri-gram, and four-gram features\nx_bigram = Conv1D(filters=nb_filters, kernel_size=2, padding=\"same\", activation=\"relu\")(inputs)\nx_bigram = GlobalMaxPooling1D()(x_bigram)\n\nx_trigram = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\nx_trigram = GlobalMaxPooling1D()(x_trigram)\n\nx_fourgram = Conv1D(filters=nb_filters, kernel_size=4, padding=\"same\", activation=\"relu\")(inputs)\nx_fourgram = GlobalMaxPooling1D()(x_fourgram)\n\n# Concatenate pooled features from different kernel sizes\nmerged = Concatenate(axis=-1)([x_bigram, x_trigram, x_fourgram])  # Shape: (batch_size, 3 * nb_filters)\n\n# Dense layer for feature processing after concatenation\nmerged = Dense(256, activation=\"relu\")(merged)\nmerged = Dropout(rate=0.2)(merged)\n\n# Reshape to add an extra dimension for MultiHeadAttention compatibility\nmerged_expanded = Reshape((1, 256))(merged)  # Shape: (batch_size, 1, 256)\n\n# Multi-head attention layer\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(merged_expanded, merged_expanded)\nattention_output = LayerNormalization()(attention_output + merged_expanded)  # Residual connection\n\n# Flatten the attention output\nattention_output = Flatten()(attention_output)\n\n# Final Dense layers for classification\nx = Dense(64, activation='relu')(attention_output)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,  # Limit epochs to 100 to reduce time and memory usage\n    validation_data=(X_test, y_test),\n#     callbacks=[early_stopping],\n    callbacks=[],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the multi-kernel CNN-Attention model configuration\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T05:49:06.148960Z","iopub.execute_input":"2024-11-12T05:49:06.149498Z","iopub.status.idle":"2024-11-12T06:00:27.474880Z","shell.execute_reply.started":"2024-11-12T05:49:06.149454Z","shell.execute_reply":"2024-11-12T06:00:27.473938Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 16, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1731390575.203451      97 service.cc:145] XLA service 0x7de564088f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1731390575.203513      97 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 16/472\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.0542 - loss: 3.1382  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1731390582.345081      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.0841 - loss: 2.9082 - val_accuracy: 0.4599 - val_loss: 1.6604\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.4121 - loss: 1.7939 - val_accuracy: 0.5387 - val_loss: 1.3419\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.5434 - loss: 1.3731 - val_accuracy: 0.5777 - val_loss: 1.2810\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.5794 - loss: 1.2971 - val_accuracy: 0.6432 - val_loss: 1.1291\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.6756 - loss: 0.9829 - val_accuracy: 0.6570 - val_loss: 1.1078\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7227 - loss: 0.8738 - val_accuracy: 0.6427 - val_loss: 1.1340\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7092 - loss: 0.9280 - val_accuracy: 0.6584 - val_loss: 1.1840\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7551 - loss: 0.7865 - val_accuracy: 0.6806 - val_loss: 1.1633\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8181 - loss: 0.5925 - val_accuracy: 0.6857 - val_loss: 1.2128\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8536 - loss: 0.4793 - val_accuracy: 0.6891 - val_loss: 1.2971\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8769 - loss: 0.4009 - val_accuracy: 0.6496 - val_loss: 1.2641\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7788 - loss: 0.7260 - val_accuracy: 0.6594 - val_loss: 1.2143\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8248 - loss: 0.5881 - val_accuracy: 0.6615 - val_loss: 1.3439\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8425 - loss: 0.5120 - val_accuracy: 0.6875 - val_loss: 1.3614\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8810 - loss: 0.4110 - val_accuracy: 0.6740 - val_loss: 1.4184\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9049 - loss: 0.3255 - val_accuracy: 0.6814 - val_loss: 1.4680\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9244 - loss: 0.2726 - val_accuracy: 0.6910 - val_loss: 1.7073\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9358 - loss: 0.2137 - val_accuracy: 0.6920 - val_loss: 1.6874\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9464 - loss: 0.1770 - val_accuracy: 0.6942 - val_loss: 1.8586\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9489 - loss: 0.1764 - val_accuracy: 0.6958 - val_loss: 1.8813\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9573 - loss: 0.1488 - val_accuracy: 0.6976 - val_loss: 1.9644\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9468 - loss: 0.1767 - val_accuracy: 0.6700 - val_loss: 1.3910\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8629 - loss: 0.4774 - val_accuracy: 0.6782 - val_loss: 1.3752\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8909 - loss: 0.3733 - val_accuracy: 0.6727 - val_loss: 1.6891\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9087 - loss: 0.3237 - val_accuracy: 0.6775 - val_loss: 1.6241\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9285 - loss: 0.2527 - val_accuracy: 0.6915 - val_loss: 1.5464\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9333 - loss: 0.2359 - val_accuracy: 0.6865 - val_loss: 1.7937\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9395 - loss: 0.2169 - val_accuracy: 0.6997 - val_loss: 1.6141\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9487 - loss: 0.1801 - val_accuracy: 0.6902 - val_loss: 1.9373\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9467 - loss: 0.1807 - val_accuracy: 0.6907 - val_loss: 1.9683\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9500 - loss: 0.1733 - val_accuracy: 0.7024 - val_loss: 2.0864\nEpoch 32/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9589 - loss: 0.1437 - val_accuracy: 0.6960 - val_loss: 2.1798\nEpoch 33/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9611 - loss: 0.1323 - val_accuracy: 0.6966 - val_loss: 2.1172\nEpoch 34/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9646 - loss: 0.1251 - val_accuracy: 0.6918 - val_loss: 2.4595\nEpoch 35/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9654 - loss: 0.1152 - val_accuracy: 0.7019 - val_loss: 2.5447\nEpoch 36/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9665 - loss: 0.1155 - val_accuracy: 0.7050 - val_loss: 2.5051\nEpoch 37/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9691 - loss: 0.1008 - val_accuracy: 0.7019 - val_loss: 2.5955\nEpoch 38/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9712 - loss: 0.0994 - val_accuracy: 0.7037 - val_loss: 2.5958\nEpoch 39/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9697 - loss: 0.1013 - val_accuracy: 0.7021 - val_loss: 2.6955\nEpoch 40/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.0998 - val_accuracy: 0.7058 - val_loss: 2.6705\nEpoch 41/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9707 - loss: 0.1004 - val_accuracy: 0.7061 - val_loss: 2.7123\nEpoch 42/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.0982 - val_accuracy: 0.7066 - val_loss: 2.7489\nEpoch 43/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9695 - loss: 0.1010 - val_accuracy: 0.7064 - val_loss: 2.7715\nEpoch 44/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9429 - loss: 0.2012 - val_accuracy: 0.6594 - val_loss: 1.6458\nEpoch 45/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9082 - loss: 0.3292 - val_accuracy: 0.6748 - val_loss: 1.7030\nEpoch 46/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9344 - loss: 0.2318 - val_accuracy: 0.6849 - val_loss: 1.8528\nEpoch 47/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9463 - loss: 0.1854 - val_accuracy: 0.6894 - val_loss: 1.7420\nEpoch 48/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9498 - loss: 0.1775 - val_accuracy: 0.6857 - val_loss: 2.0038\nEpoch 49/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9486 - loss: 0.1810 - val_accuracy: 0.6889 - val_loss: 1.9274\nEpoch 50/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9495 - loss: 0.1660 - val_accuracy: 0.6968 - val_loss: 1.9677\nEpoch 51/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9552 - loss: 0.1574 - val_accuracy: 0.6971 - val_loss: 1.9413\nEpoch 52/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9618 - loss: 0.1329 - val_accuracy: 0.6926 - val_loss: 2.0995\nEpoch 53/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9537 - loss: 0.1620 - val_accuracy: 0.6918 - val_loss: 2.1981\nEpoch 54/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9637 - loss: 0.1297 - val_accuracy: 0.6918 - val_loss: 2.3333\nEpoch 55/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9645 - loss: 0.1253 - val_accuracy: 0.6822 - val_loss: 2.2538\nEpoch 56/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9614 - loss: 0.1431 - val_accuracy: 0.6865 - val_loss: 2.3427\nEpoch 57/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9638 - loss: 0.1318 - val_accuracy: 0.6910 - val_loss: 2.3238\nEpoch 58/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9603 - loss: 0.1328 - val_accuracy: 0.6944 - val_loss: 2.3586\nEpoch 59/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9654 - loss: 0.1223 - val_accuracy: 0.7027 - val_loss: 2.2547\nEpoch 60/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9690 - loss: 0.1115 - val_accuracy: 0.6958 - val_loss: 2.4597\nEpoch 61/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9674 - loss: 0.1153 - val_accuracy: 0.6995 - val_loss: 2.5484\nEpoch 62/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9680 - loss: 0.1087 - val_accuracy: 0.6936 - val_loss: 2.5300\nEpoch 63/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.1082 - val_accuracy: 0.6883 - val_loss: 2.7982\nEpoch 64/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9677 - loss: 0.1074 - val_accuracy: 0.6886 - val_loss: 2.8847\nEpoch 65/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9691 - loss: 0.1069 - val_accuracy: 0.7003 - val_loss: 2.5709\nEpoch 66/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9716 - loss: 0.0953 - val_accuracy: 0.6820 - val_loss: 2.9850\nEpoch 67/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9664 - loss: 0.1133 - val_accuracy: 0.6944 - val_loss: 2.6055\nEpoch 68/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9699 - loss: 0.1037 - val_accuracy: 0.6984 - val_loss: 2.8183\nEpoch 69/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9713 - loss: 0.0954 - val_accuracy: 0.6942 - val_loss: 2.9574\nEpoch 70/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9700 - loss: 0.0993 - val_accuracy: 0.6966 - val_loss: 3.1933\nEpoch 71/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9736 - loss: 0.0894 - val_accuracy: 0.7024 - val_loss: 2.9919\nEpoch 72/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9730 - loss: 0.0915 - val_accuracy: 0.7003 - val_loss: 3.0996\nEpoch 73/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9729 - loss: 0.0907 - val_accuracy: 0.7045 - val_loss: 3.0570\nEpoch 74/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9736 - loss: 0.0863 - val_accuracy: 0.7034 - val_loss: 3.2156\nEpoch 75/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.1000 - val_accuracy: 0.7066 - val_loss: 3.1838\nEpoch 76/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9735 - loss: 0.0907 - val_accuracy: 0.7080 - val_loss: 3.1435\nEpoch 77/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9732 - loss: 0.0916 - val_accuracy: 0.7085 - val_loss: 3.2096\nEpoch 78/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9731 - loss: 0.0882 - val_accuracy: 0.7066 - val_loss: 3.2275\nEpoch 79/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9762 - loss: 0.0793 - val_accuracy: 0.7066 - val_loss: 3.2447\nEpoch 80/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9721 - loss: 0.0945 - val_accuracy: 0.7066 - val_loss: 3.3089\nEpoch 81/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9736 - loss: 0.0868 - val_accuracy: 0.7061 - val_loss: 3.3444\nEpoch 82/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9724 - loss: 0.0889 - val_accuracy: 0.7074 - val_loss: 3.3205\nEpoch 83/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9729 - loss: 0.0916 - val_accuracy: 0.7069 - val_loss: 3.3179\nEpoch 84/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9735 - loss: 0.0872 - val_accuracy: 0.7077 - val_loss: 3.3262\nEpoch 85/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9725 - loss: 0.0888 - val_accuracy: 0.7088 - val_loss: 3.3318\nEpoch 86/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9734 - loss: 0.0885 - val_accuracy: 0.7088 - val_loss: 3.3324\nEpoch 87/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9689 - loss: 0.1061 - val_accuracy: 0.6507 - val_loss: 1.9372\nEpoch 88/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9073 - loss: 0.3429 - val_accuracy: 0.6796 - val_loss: 2.0369\nEpoch 89/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9577 - loss: 0.1513 - val_accuracy: 0.6846 - val_loss: 1.8842\nEpoch 90/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9627 - loss: 0.1313 - val_accuracy: 0.6910 - val_loss: 2.2718\nEpoch 91/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9609 - loss: 0.1396 - val_accuracy: 0.6775 - val_loss: 2.4751\nEpoch 92/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9609 - loss: 0.1300 - val_accuracy: 0.6889 - val_loss: 2.5760\nEpoch 93/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9617 - loss: 0.1429 - val_accuracy: 0.6825 - val_loss: 2.5294\nEpoch 94/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9615 - loss: 0.1363 - val_accuracy: 0.6894 - val_loss: 2.4215\nEpoch 95/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9660 - loss: 0.1179 - val_accuracy: 0.6897 - val_loss: 2.2324\nEpoch 96/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9621 - loss: 0.1371 - val_accuracy: 0.6857 - val_loss: 2.3478\nEpoch 97/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9644 - loss: 0.1304 - val_accuracy: 0.6952 - val_loss: 2.4897\nEpoch 98/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9628 - loss: 0.1242 - val_accuracy: 0.6989 - val_loss: 2.5874\nEpoch 99/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9677 - loss: 0.1102 - val_accuracy: 0.6735 - val_loss: 3.0814\nEpoch 100/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9657 - loss: 0.1242 - val_accuracy: 0.6806 - val_loss: 2.7932\nCompleted training with the multi-kernel CNN-Attention model configuration\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T06:00:27.477629Z","iopub.execute_input":"2024-11-12T06:00:27.478144Z","iopub.status.idle":"2024-11-12T06:00:31.903370Z","shell.execute_reply.started":"2024-11-12T06:00:27.478098Z","shell.execute_reply":"2024-11-12T06:00:31.902424Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6713 - loss: 2.9162\nTest Loss: 2.793219804763794\nTest Accuracy: 0.6806365847587585\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Input, LayerNormalization, Concatenate, BatchNormalization, Reshape, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for each Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the fixed best parameters\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# Multiple Conv1D layers with different kernel sizes for bi-gram, tri-gram, and four-gram features\nx_bigram = Conv1D(filters=nb_filters, kernel_size=2, padding=\"same\", activation=\"relu\")(inputs)\nx_bigram = BatchNormalization()(x_bigram)\nx_bigram = GlobalMaxPooling1D()(x_bigram)  # Max pooling over bigrams\n\nx_trigram = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\nx_trigram = BatchNormalization()(x_trigram)\nx_trigram = GlobalMaxPooling1D()(x_trigram)  # Max pooling over trigrams\n\nx_fourgram = Conv1D(filters=nb_filters, kernel_size=4, padding=\"same\", activation=\"relu\")(inputs)\nx_fourgram = BatchNormalization()(x_fourgram)\nx_fourgram = GlobalMaxPooling1D()(x_fourgram)  # Max pooling over fourgrams\n\n# Concatenate pooled features from different kernel sizes\nmerged = Concatenate(axis=-1)([x_bigram, x_trigram, x_fourgram])  # Shape: (batch_size, 3 * nb_filters)\n\n# Dense layer for feature processing after concatenation\nmerged = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(merged)\nmerged = Dropout(rate=0.3)(merged)\n\n# Reshape for MultiHeadAttention compatibility\nmerged_expanded = Reshape((1, 256))(merged)  # Shape: (batch_size, 1, 256)\n\n# Multi-head attention layer\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(merged_expanded, merged_expanded)\nattention_output = LayerNormalization()(attention_output + merged_expanded)  # Residual connection\n\n# Flatten the attention output\nattention_output = Flatten()(attention_output)\n\n# Final Dense layers for classification\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(attention_output)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the optimized multi-kernel CNN-Attention model configuration including max pooling\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T06:20:48.934507Z","iopub.execute_input":"2024-11-12T06:20:48.934892Z","iopub.status.idle":"2024-11-12T06:26:31.732727Z","shell.execute_reply.started":"2024-11-12T06:20:48.934858Z","shell.execute_reply":"2024-11-12T06:26:31.731725Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.0984 - loss: 3.4263 - val_accuracy: 0.4944 - val_loss: 2.0955\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.4347 - loss: 2.1976 - val_accuracy: 0.5732 - val_loss: 1.6766\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.5609 - loss: 1.7557 - val_accuracy: 0.5867 - val_loss: 1.6224\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.5933 - loss: 1.6516 - val_accuracy: 0.6377 - val_loss: 1.4665\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.6702 - loss: 1.3518 - val_accuracy: 0.6549 - val_loss: 1.4411\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.7005 - loss: 1.2498 - val_accuracy: 0.6406 - val_loss: 1.4862\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.6987 - loss: 1.2695 - val_accuracy: 0.6340 - val_loss: 1.4615\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7378 - loss: 1.0906 - val_accuracy: 0.6615 - val_loss: 1.4161\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.7878 - loss: 0.9309 - val_accuracy: 0.6650 - val_loss: 1.4458\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8248 - loss: 0.8017 - val_accuracy: 0.6740 - val_loss: 1.4725\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8471 - loss: 0.7233 - val_accuracy: 0.6496 - val_loss: 1.5392\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.7735 - loss: 0.9854 - val_accuracy: 0.6520 - val_loss: 1.4412\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.7902 - loss: 0.9009 - val_accuracy: 0.6666 - val_loss: 1.4066\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8156 - loss: 0.8039 - val_accuracy: 0.6597 - val_loss: 1.5155\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8422 - loss: 0.7219 - val_accuracy: 0.6711 - val_loss: 1.4865\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8832 - loss: 0.5861 - val_accuracy: 0.6772 - val_loss: 1.4467\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9028 - loss: 0.5026 - val_accuracy: 0.6737 - val_loss: 1.6079\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9278 - loss: 0.4336 - val_accuracy: 0.6788 - val_loss: 1.7560\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9334 - loss: 0.3952 - val_accuracy: 0.6790 - val_loss: 1.8256\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9469 - loss: 0.3526 - val_accuracy: 0.6833 - val_loss: 1.8742\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9506 - loss: 0.3400 - val_accuracy: 0.6838 - val_loss: 1.9074\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.3423 - val_accuracy: 0.6302 - val_loss: 1.5982\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8383 - loss: 0.7054 - val_accuracy: 0.6578 - val_loss: 1.5302\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8667 - loss: 0.6095 - val_accuracy: 0.6682 - val_loss: 1.5465\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8945 - loss: 0.5166 - val_accuracy: 0.6639 - val_loss: 1.6784\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8922 - loss: 0.5225 - val_accuracy: 0.6706 - val_loss: 1.7095\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9152 - loss: 0.4420 - val_accuracy: 0.6703 - val_loss: 1.6733\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9201 - loss: 0.4099 - val_accuracy: 0.6658 - val_loss: 1.7821\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9356 - loss: 0.3636 - val_accuracy: 0.6751 - val_loss: 1.7368\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9415 - loss: 0.3363 - val_accuracy: 0.6806 - val_loss: 1.8851\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9470 - loss: 0.3158 - val_accuracy: 0.6870 - val_loss: 1.9348\nEpoch 32/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9474 - loss: 0.3015 - val_accuracy: 0.6859 - val_loss: 1.9855\nEpoch 33/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9520 - loss: 0.2913 - val_accuracy: 0.6775 - val_loss: 2.0436\nEpoch 34/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9594 - loss: 0.2600 - val_accuracy: 0.6894 - val_loss: 2.1021\nEpoch 35/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9609 - loss: 0.2450 - val_accuracy: 0.6788 - val_loss: 2.1691\nEpoch 36/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9679 - loss: 0.2268 - val_accuracy: 0.6950 - val_loss: 2.1964\nEpoch 37/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9673 - loss: 0.2225 - val_accuracy: 0.6806 - val_loss: 2.2873\nEpoch 38/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9689 - loss: 0.2158 - val_accuracy: 0.6955 - val_loss: 2.2741\nEpoch 39/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9694 - loss: 0.2047 - val_accuracy: 0.6886 - val_loss: 2.2822\nEpoch 40/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9716 - loss: 0.1968 - val_accuracy: 0.6923 - val_loss: 2.2995\nEpoch 41/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9707 - loss: 0.1993 - val_accuracy: 0.6910 - val_loss: 2.3046\nEpoch 42/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9747 - loss: 0.1895 - val_accuracy: 0.6910 - val_loss: 2.3142\nEpoch 43/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9708 - loss: 0.1953 - val_accuracy: 0.6912 - val_loss: 2.3146\nEpoch 44/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9457 - loss: 0.2868 - val_accuracy: 0.6374 - val_loss: 1.8198\nEpoch 45/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8816 - loss: 0.5430 - val_accuracy: 0.6708 - val_loss: 1.7021\nEpoch 46/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.4281 - val_accuracy: 0.6698 - val_loss: 1.6957\nEpoch 47/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9354 - loss: 0.3479 - val_accuracy: 0.6729 - val_loss: 1.8675\nEpoch 48/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9303 - loss: 0.3620 - val_accuracy: 0.6767 - val_loss: 1.7984\nCompleted training with the optimized multi-kernel CNN-Attention model configuration including max pooling\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T06:15:15.620133Z","iopub.status.idle":"2024-11-12T06:15:15.620490Z","shell.execute_reply.started":"2024-11-12T06:15:15.620310Z","shell.execute_reply":"2024-11-12T06:15:15.620328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN-LSTM-Attention","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, Conv1D, GlobalMaxPooling1D, LSTM, Input, LayerNormalization, Concatenate, BatchNormalization, Reshape, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for each Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the fixed best parameters\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# Multiple Conv1D layers with different kernel sizes for bi-gram, tri-gram, and four-gram features\nx_bigram = Conv1D(filters=nb_filters, kernel_size=2, padding=\"same\", activation=\"relu\")(inputs)\nx_bigram = BatchNormalization()(x_bigram)\nx_bigram = GlobalMaxPooling1D()(x_bigram)  # Max pooling over bigrams\n\nx_trigram = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\nx_trigram = BatchNormalization()(x_trigram)\nx_trigram = GlobalMaxPooling1D()(x_trigram)  # Max pooling over trigrams\n\nx_fourgram = Conv1D(filters=nb_filters, kernel_size=4, padding=\"same\", activation=\"relu\")(inputs)\nx_fourgram = BatchNormalization()(x_fourgram)\nx_fourgram = GlobalMaxPooling1D()(x_fourgram)  # Max pooling over fourgrams\n\n# Concatenate pooled features from different kernel sizes\nmerged = Concatenate(axis=-1)([x_bigram, x_trigram, x_fourgram])  # Shape: (batch_size, 3 * nb_filters)\n\n# Reshape for LSTM compatibility\nmerged_reshaped = Reshape((3, nb_filters))(merged)  # Shape: (batch_size, 3, nb_filters)\n\n# LSTM layer for sequential feature processing\nlstm_output = LSTM(128, return_sequences=True)(merged_reshaped)\n\n# Flatten LSTM output to connect with Dense layers and attention\nflattened_lstm_output = Flatten()(lstm_output)\n\n# Dense layer for feature processing after LSTM\nx = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(flattened_lstm_output)\nx = Dropout(rate=0.3)(x)\n\n# Reshape for MultiHeadAttention compatibility\nx_expanded = Reshape((1, 256))(x)  # Shape: (batch_size, 1, 256)\n\n# Multi-head attention layer\nattention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=64)(x_expanded, x_expanded)\nattention_output = LayerNormalization()(attention_output + x_expanded)  # Residual connection\n\n# Flatten the attention output\nattention_output = Flatten()(attention_output)\n\n# Final Dense layers for classification\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(attention_output)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_data=(X_test, y_test),\n    callbacks=[],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the multi-kernel CNN-Attention-LSTM model configuration\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T08:04:55.727120Z","iopub.execute_input":"2024-11-12T08:04:55.727563Z","iopub.status.idle":"2024-11-12T08:20:56.680009Z","shell.execute_reply.started":"2024-11-12T08:04:55.727524Z","shell.execute_reply":"2024-11-12T08:20:56.679099Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.0966 - loss: 3.4476 - val_accuracy: 0.4218 - val_loss: 2.2213\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.4049 - loss: 2.2545 - val_accuracy: 0.5520 - val_loss: 1.7308\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.5361 - loss: 1.7663 - val_accuracy: 0.5759 - val_loss: 1.6686\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.5574 - loss: 1.6934 - val_accuracy: 0.6196 - val_loss: 1.4823\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.6504 - loss: 1.3677 - val_accuracy: 0.6263 - val_loss: 1.4541\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.6796 - loss: 1.2637 - val_accuracy: 0.6228 - val_loss: 1.4459\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.6535 - loss: 1.3216 - val_accuracy: 0.6347 - val_loss: 1.3956\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7056 - loss: 1.1373 - val_accuracy: 0.6485 - val_loss: 1.3766\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.7506 - loss: 0.9675 - val_accuracy: 0.6621 - val_loss: 1.3605\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8083 - loss: 0.7934 - val_accuracy: 0.6602 - val_loss: 1.4013\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8236 - loss: 0.7320 - val_accuracy: 0.6321 - val_loss: 1.4378\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.7297 - loss: 1.0159 - val_accuracy: 0.6374 - val_loss: 1.4769\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.7440 - loss: 0.9932 - val_accuracy: 0.6406 - val_loss: 1.4584\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.7899 - loss: 0.8213 - val_accuracy: 0.6395 - val_loss: 1.4852\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8067 - loss: 0.7648 - val_accuracy: 0.6528 - val_loss: 1.4437\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8514 - loss: 0.6234 - val_accuracy: 0.6562 - val_loss: 1.4805\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8777 - loss: 0.5249 - val_accuracy: 0.6538 - val_loss: 1.6154\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8979 - loss: 0.4590 - val_accuracy: 0.6666 - val_loss: 1.6837\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9201 - loss: 0.3969 - val_accuracy: 0.6706 - val_loss: 1.7840\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9351 - loss: 0.3391 - val_accuracy: 0.6721 - val_loss: 1.8453\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9411 - loss: 0.3236 - val_accuracy: 0.6724 - val_loss: 1.8582\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9358 - loss: 0.3412 - val_accuracy: 0.6265 - val_loss: 1.6955\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8155 - loss: 0.7473 - val_accuracy: 0.6451 - val_loss: 1.4774\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8411 - loss: 0.6471 - val_accuracy: 0.6483 - val_loss: 1.4467\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8490 - loss: 0.6263 - val_accuracy: 0.6586 - val_loss: 1.5270\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8751 - loss: 0.5491 - val_accuracy: 0.6520 - val_loss: 1.6161\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8941 - loss: 0.4646 - val_accuracy: 0.6531 - val_loss: 1.6744\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8997 - loss: 0.4412 - val_accuracy: 0.6584 - val_loss: 1.6585\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9147 - loss: 0.3937 - val_accuracy: 0.6594 - val_loss: 1.6120\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9252 - loss: 0.3607 - val_accuracy: 0.6584 - val_loss: 1.6920\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9329 - loss: 0.3194 - val_accuracy: 0.6653 - val_loss: 1.8513\nEpoch 32/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9450 - loss: 0.2873 - val_accuracy: 0.6565 - val_loss: 1.8982\nEpoch 33/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9457 - loss: 0.2775 - val_accuracy: 0.6727 - val_loss: 2.0353\nEpoch 34/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9532 - loss: 0.2467 - val_accuracy: 0.6671 - val_loss: 1.9761\nEpoch 35/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9569 - loss: 0.2304 - val_accuracy: 0.6732 - val_loss: 2.0192\nEpoch 36/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9625 - loss: 0.2135 - val_accuracy: 0.6684 - val_loss: 2.1622\nEpoch 37/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9650 - loss: 0.1991 - val_accuracy: 0.6708 - val_loss: 2.1695\nEpoch 38/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9700 - loss: 0.1854 - val_accuracy: 0.6690 - val_loss: 2.2605\nEpoch 39/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9680 - loss: 0.1853 - val_accuracy: 0.6721 - val_loss: 2.2815\nEpoch 40/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9697 - loss: 0.1761 - val_accuracy: 0.6698 - val_loss: 2.3151\nEpoch 41/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9707 - loss: 0.1754 - val_accuracy: 0.6767 - val_loss: 2.2984\nEpoch 42/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9731 - loss: 0.1660 - val_accuracy: 0.6743 - val_loss: 2.3023\nEpoch 43/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9725 - loss: 0.1677 - val_accuracy: 0.6775 - val_loss: 2.3049\nEpoch 44/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9382 - loss: 0.2867 - val_accuracy: 0.6546 - val_loss: 1.4552\nEpoch 45/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8580 - loss: 0.5763 - val_accuracy: 0.6477 - val_loss: 1.6095\nEpoch 46/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8928 - loss: 0.4526 - val_accuracy: 0.6695 - val_loss: 1.6876\nEpoch 47/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9052 - loss: 0.4277 - val_accuracy: 0.6560 - val_loss: 1.7261\nEpoch 48/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9139 - loss: 0.3860 - val_accuracy: 0.6674 - val_loss: 1.7734\nEpoch 49/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9323 - loss: 0.3296 - val_accuracy: 0.6639 - val_loss: 1.7401\nEpoch 50/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9306 - loss: 0.3224 - val_accuracy: 0.6679 - val_loss: 1.8272\nEpoch 51/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9379 - loss: 0.3095 - val_accuracy: 0.6605 - val_loss: 1.7639\nEpoch 52/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9362 - loss: 0.3053 - val_accuracy: 0.6793 - val_loss: 1.9645\nEpoch 53/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9221 - loss: 0.3493 - val_accuracy: 0.6721 - val_loss: 1.8547\nEpoch 54/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9442 - loss: 0.2823 - val_accuracy: 0.6761 - val_loss: 1.9527\nEpoch 55/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9452 - loss: 0.2636 - val_accuracy: 0.6711 - val_loss: 1.9016\nEpoch 56/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9439 - loss: 0.2670 - val_accuracy: 0.6743 - val_loss: 1.9221\nEpoch 57/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9480 - loss: 0.2566 - val_accuracy: 0.6743 - val_loss: 1.9642\nEpoch 58/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9521 - loss: 0.2520 - val_accuracy: 0.6655 - val_loss: 2.0882\nEpoch 59/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9533 - loss: 0.2368 - val_accuracy: 0.6695 - val_loss: 1.9601\nEpoch 60/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9540 - loss: 0.2360 - val_accuracy: 0.6674 - val_loss: 2.1254\nEpoch 61/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9555 - loss: 0.2353 - val_accuracy: 0.6775 - val_loss: 1.9961\nEpoch 62/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9619 - loss: 0.2034 - val_accuracy: 0.6793 - val_loss: 1.9583\nEpoch 63/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9551 - loss: 0.2312 - val_accuracy: 0.6737 - val_loss: 2.0492\nEpoch 64/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9573 - loss: 0.2150 - val_accuracy: 0.6637 - val_loss: 2.2114\nEpoch 65/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9673 - loss: 0.1790 - val_accuracy: 0.6684 - val_loss: 2.1551\nEpoch 66/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9610 - loss: 0.1987 - val_accuracy: 0.6804 - val_loss: 2.2321\nEpoch 67/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9657 - loss: 0.1828 - val_accuracy: 0.6825 - val_loss: 2.2138\nEpoch 68/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9711 - loss: 0.1625 - val_accuracy: 0.6812 - val_loss: 2.3529\nEpoch 69/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9657 - loss: 0.1731 - val_accuracy: 0.6782 - val_loss: 2.2817\nEpoch 70/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9670 - loss: 0.1680 - val_accuracy: 0.6759 - val_loss: 2.3845\nEpoch 71/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9698 - loss: 0.1620 - val_accuracy: 0.6838 - val_loss: 2.2952\nEpoch 72/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9721 - loss: 0.1501 - val_accuracy: 0.6828 - val_loss: 2.3197\nEpoch 73/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9723 - loss: 0.1471 - val_accuracy: 0.6790 - val_loss: 2.3970\nEpoch 74/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9731 - loss: 0.1418 - val_accuracy: 0.6812 - val_loss: 2.3370\nEpoch 75/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9705 - loss: 0.1487 - val_accuracy: 0.6833 - val_loss: 2.3411\nEpoch 76/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9740 - loss: 0.1380 - val_accuracy: 0.6846 - val_loss: 2.3764\nEpoch 77/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9728 - loss: 0.1383 - val_accuracy: 0.6812 - val_loss: 2.3869\nEpoch 78/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9747 - loss: 0.1316 - val_accuracy: 0.6859 - val_loss: 2.3698\nEpoch 79/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9727 - loss: 0.1387 - val_accuracy: 0.6836 - val_loss: 2.3930\nEpoch 80/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9743 - loss: 0.1312 - val_accuracy: 0.6844 - val_loss: 2.4036\nEpoch 81/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9747 - loss: 0.1282 - val_accuracy: 0.6846 - val_loss: 2.4199\nEpoch 82/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9741 - loss: 0.1292 - val_accuracy: 0.6838 - val_loss: 2.4320\nEpoch 83/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9749 - loss: 0.1286 - val_accuracy: 0.6849 - val_loss: 2.4183\nEpoch 84/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9739 - loss: 0.1295 - val_accuracy: 0.6857 - val_loss: 2.4182\nEpoch 85/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9752 - loss: 0.1236 - val_accuracy: 0.6857 - val_loss: 2.4152\nEpoch 86/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9735 - loss: 0.1329 - val_accuracy: 0.6859 - val_loss: 2.4238\nEpoch 87/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9698 - loss: 0.1440 - val_accuracy: 0.6244 - val_loss: 1.8089\nEpoch 88/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8538 - loss: 0.5931 - val_accuracy: 0.6599 - val_loss: 1.6609\nEpoch 89/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9192 - loss: 0.3672 - val_accuracy: 0.6565 - val_loss: 1.8169\nEpoch 90/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9328 - loss: 0.3198 - val_accuracy: 0.6645 - val_loss: 1.8000\nEpoch 91/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9318 - loss: 0.3157 - val_accuracy: 0.6690 - val_loss: 1.8609\nEpoch 92/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9376 - loss: 0.3024 - val_accuracy: 0.6639 - val_loss: 1.8944\nEpoch 93/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9479 - loss: 0.2517 - val_accuracy: 0.6653 - val_loss: 1.9210\nEpoch 94/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9530 - loss: 0.2395 - val_accuracy: 0.6666 - val_loss: 2.2175\nEpoch 95/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9517 - loss: 0.2393 - val_accuracy: 0.6610 - val_loss: 2.1649\nEpoch 96/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9456 - loss: 0.2679 - val_accuracy: 0.6645 - val_loss: 1.9826\nEpoch 97/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9473 - loss: 0.2591 - val_accuracy: 0.6737 - val_loss: 2.1907\nEpoch 98/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9549 - loss: 0.2350 - val_accuracy: 0.6668 - val_loss: 1.9987\nEpoch 99/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9462 - loss: 0.2677 - val_accuracy: 0.6745 - val_loss: 2.0090\nEpoch 100/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9481 - loss: 0.2553 - val_accuracy: 0.6732 - val_loss: 1.8800\nCompleted training with the multi-kernel CNN-Attention-LSTM model configuration\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T08:20:56.683381Z","iopub.execute_input":"2024-11-12T08:20:56.683678Z","iopub.status.idle":"2024-11-12T08:21:01.418657Z","shell.execute_reply.started":"2024-11-12T08:20:56.683647Z","shell.execute_reply":"2024-11-12T08:21:01.417778Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6646 - loss: 1.9677\nTest Loss: 1.8799532651901245\nTest Accuracy: 0.673209547996521\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM-CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Input, LayerNormalization, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for the Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the BiLSTM and CNN\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# BiLSTM layer for sequential feature processing\nbilstm_output = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n\n# Conv1D layer after BiLSTM to extract local features\nconv_output = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(bilstm_output)\nconv_output = GlobalMaxPooling1D()(conv_output)  # Global max pooling to reduce sequence dimension\n\n# Dense layer for feature processing after convolution\nx = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(conv_output)\nx = Dropout(rate=0.3)(x)\n\n# Final Dense layers for classification\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.0005,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the BiLSTM-CNN model configuration\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T09:34:37.489288Z","iopub.execute_input":"2024-11-12T09:34:37.489770Z","iopub.status.idle":"2024-11-12T09:41:58.599160Z","shell.execute_reply.started":"2024-11-12T09:34:37.489726Z","shell.execute_reply":"2024-11-12T09:41:58.598171Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.1498 - loss: 3.0878 - val_accuracy: 0.5268 - val_loss: 1.7044\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.4791 - loss: 1.8461 - val_accuracy: 0.6114 - val_loss: 1.4515\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.5962 - loss: 1.5152 - val_accuracy: 0.6294 - val_loss: 1.3984\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - accuracy: 0.6219 - loss: 1.4262 - val_accuracy: 0.6454 - val_loss: 1.3290\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.6862 - loss: 1.2133 - val_accuracy: 0.6605 - val_loss: 1.3035\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.7047 - loss: 1.1434 - val_accuracy: 0.6406 - val_loss: 1.4369\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.7024 - loss: 1.1423 - val_accuracy: 0.6687 - val_loss: 1.3126\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - accuracy: 0.7567 - loss: 0.9628 - val_accuracy: 0.6846 - val_loss: 1.2866\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8051 - loss: 0.8090 - val_accuracy: 0.6973 - val_loss: 1.2898\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8385 - loss: 0.7039 - val_accuracy: 0.6997 - val_loss: 1.3075\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8543 - loss: 0.6545 - val_accuracy: 0.6865 - val_loss: 1.3503\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.7987 - loss: 0.8039 - val_accuracy: 0.6806 - val_loss: 1.2981\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8326 - loss: 0.7003 - val_accuracy: 0.6830 - val_loss: 1.4206\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8442 - loss: 0.6713 - val_accuracy: 0.6883 - val_loss: 1.3729\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8973 - loss: 0.4944 - val_accuracy: 0.6989 - val_loss: 1.4413\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9162 - loss: 0.4234 - val_accuracy: 0.6963 - val_loss: 1.6166\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9345 - loss: 0.3611 - val_accuracy: 0.6960 - val_loss: 1.6048\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9475 - loss: 0.3229 - val_accuracy: 0.6992 - val_loss: 1.7143\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9580 - loss: 0.2888 - val_accuracy: 0.7024 - val_loss: 1.7821\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9618 - loss: 0.2669 - val_accuracy: 0.7016 - val_loss: 1.8240\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9649 - loss: 0.2591 - val_accuracy: 0.7048 - val_loss: 1.8401\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9592 - loss: 0.2751 - val_accuracy: 0.6615 - val_loss: 1.4145\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.8768 - loss: 0.5393 - val_accuracy: 0.6939 - val_loss: 1.5479\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9192 - loss: 0.3924 - val_accuracy: 0.6920 - val_loss: 1.5630\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9360 - loss: 0.3408 - val_accuracy: 0.6836 - val_loss: 1.6613\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9355 - loss: 0.3489 - val_accuracy: 0.6873 - val_loss: 1.6334\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9446 - loss: 0.3046 - val_accuracy: 0.6950 - val_loss: 1.6485\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9540 - loss: 0.2684 - val_accuracy: 0.6905 - val_loss: 1.7483\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - accuracy: 0.9583 - loss: 0.2495 - val_accuracy: 0.7045 - val_loss: 1.7520\nEpoch 30/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9608 - loss: 0.2345 - val_accuracy: 0.7005 - val_loss: 1.8289\nEpoch 31/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.9654 - loss: 0.2121 - val_accuracy: 0.6989 - val_loss: 1.8662\nCompleted training with the BiLSTM-CNN model configuration\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T09:41:58.600298Z","iopub.execute_input":"2024-11-12T09:41:58.600606Z","iopub.status.idle":"2024-11-12T09:42:03.680199Z","shell.execute_reply.started":"2024-11-12T09:41:58.600573Z","shell.execute_reply":"2024-11-12T09:42:03.679285Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7037 - loss: 1.9077\nTest Loss: 1.8400700092315674\nTest Accuracy: 0.7047745585441589\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM-CNN-Attention","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Input, LayerNormalization, Flatten, Attention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for the Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the BiLSTM, CNN, Attention, and Layer Normalization\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# BiLSTM layer for sequential feature processing\nbilstm_output = Bidirectional(LSTM(128, return_sequences=True))(inputs)\nbilstm_output = LayerNormalization()(bilstm_output)  # Apply Layer Normalization after BiLSTM\n\n# Add Attention layer after BiLSTM to focus on relevant sequence information\nattention_output = Attention()([bilstm_output, bilstm_output])\n\n# Conv1D layer after Attention to extract local features\nconv_output = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(attention_output)\nconv_output = GlobalMaxPooling1D()(conv_output)  # Global max pooling to reduce sequence dimension\nconv_output = LayerNormalization()(conv_output)  # Apply Layer Normalization after Conv1D\n\n# Dense layer for feature processing after convolution\nx = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(conv_output)\nx = Dropout(rate=0.3)(x)\n\n# Additional Dense layers for classification with regularization and Dropout\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.001,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the BiLSTM-CNN model configuration with Attention and Layer Normalization\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T06:34:51.266417Z","iopub.execute_input":"2024-11-13T06:34:51.266726Z","iopub.status.idle":"2024-11-13T06:42:26.064980Z","shell.execute_reply.started":"2024-11-13T06:34:51.266694Z","shell.execute_reply":"2024-11-13T06:42:26.064015Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.1295 - loss: 3.1364 - val_accuracy: 0.5276 - val_loss: 1.6934\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.5030 - loss: 1.7870 - val_accuracy: 0.6332 - val_loss: 1.4125\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.6294 - loss: 1.4138 - val_accuracy: 0.6186 - val_loss: 1.4244\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.6381 - loss: 1.3692 - val_accuracy: 0.6538 - val_loss: 1.2750\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.7169 - loss: 1.0964 - val_accuracy: 0.6841 - val_loss: 1.2351\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.7524 - loss: 0.9814 - val_accuracy: 0.6377 - val_loss: 1.3558\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.7206 - loss: 1.0836 - val_accuracy: 0.6886 - val_loss: 1.2079\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.7803 - loss: 0.8771 - val_accuracy: 0.6939 - val_loss: 1.2409\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8421 - loss: 0.6537 - val_accuracy: 0.7027 - val_loss: 1.2802\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8910 - loss: 0.4977 - val_accuracy: 0.7056 - val_loss: 1.3465\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9118 - loss: 0.4413 - val_accuracy: 0.6581 - val_loss: 1.3902\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8008 - loss: 0.7817 - val_accuracy: 0.6889 - val_loss: 1.2743\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8495 - loss: 0.6401 - val_accuracy: 0.6889 - val_loss: 1.4540\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8812 - loss: 0.5219 - val_accuracy: 0.6958 - val_loss: 1.3724\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9061 - loss: 0.4390 - val_accuracy: 0.6915 - val_loss: 1.4832\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9270 - loss: 0.3658 - val_accuracy: 0.7072 - val_loss: 1.6006\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9490 - loss: 0.2845 - val_accuracy: 0.7058 - val_loss: 1.6697\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9614 - loss: 0.2391 - val_accuracy: 0.7125 - val_loss: 1.7407\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9668 - loss: 0.2172 - val_accuracy: 0.7162 - val_loss: 1.7726\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9698 - loss: 0.2036 - val_accuracy: 0.7159 - val_loss: 1.8156\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9707 - loss: 0.1964 - val_accuracy: 0.7135 - val_loss: 1.8356\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9655 - loss: 0.2157 - val_accuracy: 0.6692 - val_loss: 1.4753\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8372 - loss: 0.6758 - val_accuracy: 0.6886 - val_loss: 1.3977\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.8947 - loss: 0.4879 - val_accuracy: 0.6923 - val_loss: 1.4150\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9222 - loss: 0.3962 - val_accuracy: 0.6918 - val_loss: 1.6191\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9368 - loss: 0.3339 - val_accuracy: 0.6865 - val_loss: 1.7895\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9437 - loss: 0.3026 - val_accuracy: 0.6902 - val_loss: 1.6710\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9490 - loss: 0.2791 - val_accuracy: 0.6992 - val_loss: 1.6645\nEpoch 29/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.9539 - loss: 0.2497 - val_accuracy: 0.6891 - val_loss: 1.8656\nCompleted training with the BiLSTM-CNN model configuration with Attention and Layer Normalization\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T06:42:26.067155Z","iopub.execute_input":"2024-11-13T06:42:26.067515Z","iopub.status.idle":"2024-11-13T06:42:31.274238Z","shell.execute_reply.started":"2024-11-13T06:42:26.067480Z","shell.execute_reply":"2024-11-13T06:42:31.273348Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7098 - loss: 1.8359\nTest Loss: 1.7726138830184937\nTest Accuracy: 0.7161803841590881\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM-CNN-Multihead Attention","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Input, LayerNormalization, Flatten, MultiHeadAttention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nimport numpy as np\nimport gc\n\n# Model parameters\nmax_sequence_length, embedding_dim = 128, 768\nnb_filters = 128  # Number of filters for the Conv1D layer\nattention_heads = 16  # Best attention heads\nfirst_decay_steps = 40  # Best first decay steps\nnum_classes = len(np.unique(y_train))  # Assuming y_train is defined with labels\n\n# Define the model with the BiLSTM, CNN, Multi-Head Attention, and Layer Normalization\ninputs = Input(shape=(max_sequence_length, embedding_dim))\n\n# BiLSTM layer for sequential feature processing\nbilstm_output = Bidirectional(LSTM(128, return_sequences=True))(inputs)\nbilstm_output = LayerNormalization()(bilstm_output)  # Apply Layer Normalization after BiLSTM\n\n# Add Multi-Head Attention after BiLSTM to focus on relevant sequence information\nmultihead_attention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=embedding_dim)(bilstm_output, bilstm_output)\nmultihead_attention_output = LayerNormalization()(multihead_attention_output)  # Layer Normalization after Multi-Head Attention\n\n# Conv1D layer after Multi-Head Attention to extract local features\nconv_output = Conv1D(filters=nb_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(multihead_attention_output)\nconv_output = GlobalMaxPooling1D()(conv_output)  # Global max pooling to reduce sequence dimension\nconv_output = LayerNormalization()(conv_output)  # Apply Layer Normalization after Conv1D\n\n# Dense layer for feature processing after convolution\nx = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(conv_output)\nx = Dropout(rate=0.3)(x)\n\n# Additional Dense layers for classification with regularization and Dropout\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = Dropout(0.3)(x)\n\n# Output layer for multi-class classification\noutputs = Dense(num_classes, activation=\"softmax\")(x)  # Softmax for multi-class\n\n# Define the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Cosine annealing learning rate schedule\ncosine_annealing = CosineDecayRestarts(\n    initial_learning_rate=0.001,\n    first_decay_steps=first_decay_steps,\n    t_mul=2,\n    alpha=0.01\n)\noptimizer = Adam(learning_rate=cosine_annealing)\n\n# Compile the model for multi-class classification\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# Clear memory after training\ntf.keras.backend.clear_session()\ngc.collect()  # Explicit garbage collection\n\n# Print completion message\nprint(\"Completed training with the BiLSTM-CNN model configuration with Multi-Head Attention and Layer Normalization\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T07:23:55.623677Z","iopub.execute_input":"2024-11-14T07:23:55.624093Z","iopub.status.idle":"2024-11-14T07:53:53.760297Z","shell.execute_reply.started":"2024-11-14T07:23:55.624060Z","shell.execute_reply":"2024-11-14T07:53:53.759293Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 142ms/step - accuracy: 0.0492 - loss: 3.3747 - val_accuracy: 0.0546 - val_loss: 3.2093\nEpoch 2/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.0522 - loss: 3.1948 - val_accuracy: 0.0584 - val_loss: 3.1163\nEpoch 3/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.0667 - loss: 3.0785 - val_accuracy: 0.1515 - val_loss: 2.6216\nEpoch 4/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.1849 - loss: 2.3768 - val_accuracy: 0.3241 - val_loss: 1.8826\nEpoch 5/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.3108 - loss: 1.8985 - val_accuracy: 0.3902 - val_loss: 1.7397\nEpoch 6/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.3570 - loss: 1.7851 - val_accuracy: 0.4199 - val_loss: 1.6895\nEpoch 7/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.4281 - loss: 1.6905 - val_accuracy: 0.5090 - val_loss: 1.4555\nEpoch 8/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.5114 - loss: 1.4795 - val_accuracy: 0.5398 - val_loss: 1.3581\nEpoch 9/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.5675 - loss: 1.3048 - val_accuracy: 0.5655 - val_loss: 1.3201\nEpoch 10/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6093 - loss: 1.2113 - val_accuracy: 0.5931 - val_loss: 1.3061\nEpoch 11/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6184 - loss: 1.1609 - val_accuracy: 0.5485 - val_loss: 1.4385\nEpoch 12/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.5738 - loss: 1.3178 - val_accuracy: 0.5706 - val_loss: 1.3474\nEpoch 13/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.5973 - loss: 1.2542 - val_accuracy: 0.6005 - val_loss: 1.3023\nEpoch 14/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6290 - loss: 1.1665 - val_accuracy: 0.6069 - val_loss: 1.2874\nEpoch 15/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6461 - loss: 1.1052 - val_accuracy: 0.6114 - val_loss: 1.3577\nEpoch 16/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6754 - loss: 1.0400 - val_accuracy: 0.6218 - val_loss: 1.3038\nEpoch 17/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6974 - loss: 0.9555 - val_accuracy: 0.6220 - val_loss: 1.3421\nEpoch 18/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7215 - loss: 0.8777 - val_accuracy: 0.6385 - val_loss: 1.3731\nEpoch 19/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7459 - loss: 0.8149 - val_accuracy: 0.6279 - val_loss: 1.4730\nEpoch 20/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7672 - loss: 0.7618 - val_accuracy: 0.6355 - val_loss: 1.4860\nEpoch 21/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7824 - loss: 0.7015 - val_accuracy: 0.6340 - val_loss: 1.5268\nEpoch 22/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7826 - loss: 0.7105 - val_accuracy: 0.5846 - val_loss: 1.4352\nEpoch 23/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.6989 - loss: 0.9868 - val_accuracy: 0.6225 - val_loss: 1.3865\nEpoch 24/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7043 - loss: 0.9645 - val_accuracy: 0.6321 - val_loss: 1.3438\nEpoch 25/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7248 - loss: 0.8843 - val_accuracy: 0.6249 - val_loss: 1.4876\nEpoch 26/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7262 - loss: 0.8703 - val_accuracy: 0.6313 - val_loss: 1.3606\nEpoch 27/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7502 - loss: 0.8245 - val_accuracy: 0.6332 - val_loss: 1.4167\nEpoch 28/100\n\u001b[1m472/472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7596 - loss: 0.7875 - val_accuracy: 0.6284 - val_loss: 1.4950\nCompleted training with the BiLSTM-CNN model configuration with Multi-Head Attention and Layer Normalization\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T07:17:06.501380Z","iopub.status.idle":"2024-11-14T07:17:06.501753Z","shell.execute_reply.started":"2024-11-14T07:17:06.501560Z","shell.execute_reply":"2024-11-14T07:17:06.501578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}