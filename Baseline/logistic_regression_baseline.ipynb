{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T07:18:58.067332Z","iopub.execute_input":"2024-09-30T07:18:58.068054Z","iopub.status.idle":"2024-09-30T07:18:58.073868Z","shell.execute_reply.started":"2024-09-30T07:18:58.068003Z","shell.execute_reply":"2024-09-30T07:18:58.072838Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nnewsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:41:17.698777Z","iopub.execute_input":"2024-09-30T06:41:17.699852Z","iopub.status.idle":"2024-09-30T06:41:43.386032Z","shell.execute_reply.started":"2024-09-30T06:41:17.699805Z","shell.execute_reply":"2024-09-30T06:41:43.385112Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Text': newsgroups.data,\n    'Category': newsgroups.target\n})\ndf['Category Name'] = df['Category'].apply(lambda x: newsgroups.target_names[x])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:41:43.387627Z","iopub.execute_input":"2024-09-30T06:41:43.387975Z","iopub.status.idle":"2024-09-30T06:41:43.433011Z","shell.execute_reply.started":"2024-09-30T06:41:43.387926Z","shell.execute_reply":"2024-09-30T06:41:43.432001Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                    Text  Category  \\\n0      \\n\\nI am sure some bashers of Pens fans are pr...        10   \n1      My brother is in the market for a high-perform...         3   \n2      \\n\\n\\n\\n\\tFinally you said what you dream abou...        17   \n3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...         3   \n4      1)    I have an old Jasmine drive which I cann...         4   \n...                                                  ...       ...   \n18841  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...        13   \n18842  \\nNot in isolated ground recepticles (usually ...        12   \n18843  I just installed a DX2-66 CPU in a clone mothe...         3   \n18844  \\nWouldn't this require a hyper-sphere.  In 3-...         1   \n18845  After a tip from Gary Crum (crum@fcom.cc.utah....         7   \n\n                  Category Name  \n0              rec.sport.hockey  \n1      comp.sys.ibm.pc.hardware  \n2         talk.politics.mideast  \n3      comp.sys.ibm.pc.hardware  \n4         comp.sys.mac.hardware  \n...                         ...  \n18841                   sci.med  \n18842           sci.electronics  \n18843  comp.sys.ibm.pc.hardware  \n18844             comp.graphics  \n18845                 rec.autos  \n\n[18846 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Category</th>\n      <th>Category Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>My brother is in the market for a high-perform...</td>\n      <td>3</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n      <td>17</td>\n      <td>talk.politics.mideast</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n      <td>3</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1)    I have an old Jasmine drive which I cann...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18841</th>\n      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n      <td>13</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <th>18842</th>\n      <td>\\nNot in isolated ground recepticles (usually ...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n    </tr>\n    <tr>\n      <th>18843</th>\n      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n      <td>3</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>18844</th>\n      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>18845</th>\n      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n  </tbody>\n</table>\n<p>18846 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Lowercasing\ndf['Text'] = df['Text'].str.lower()\n\n# Remove Punctuation and special characters\ndf['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n\n# Tokenization\ndf['Tokens'] = df['Text'].apply(word_tokenize)\n\n# Removing Stopwords\nstop_words = set(stopwords.words('english'))\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n# Stemming\nstemmer = PorterStemmer()\ndf['Tokens'] = df['Tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:43:09.648153Z","iopub.execute_input":"2024-09-30T06:43:09.648550Z","iopub.status.idle":"2024-09-30T06:44:12.031023Z","shell.execute_reply.started":"2024-09-30T06:43:09.648516Z","shell.execute_reply":"2024-09-30T06:44:12.030127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"max(df['Tokens'].apply(lambda x: len(x)))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T06:56:43.697234Z","iopub.execute_input":"2024-09-30T06:56:43.698038Z","iopub.status.idle":"2024-09-30T06:56:43.715868Z","shell.execute_reply.started":"2024-09-30T06:56:43.697998Z","shell.execute_reply":"2024-09-30T06:56:43.714877Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"6620"},"metadata":{}}]},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"markdown","source":"## Data Preperation","metadata":{}},{"cell_type":"code","source":"# Define and fit the TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=6620, stop_words='english')  # Set max_features to limit vocab size if necessary\nX_tfidf = tfidf_vectorizer.fit_transform(df['Text']).toarray()  # Assuming you are using the original 'Text' column\n\n# Encode the labels\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['Category'])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:07:27.389228Z","iopub.execute_input":"2024-09-30T07:07:27.390206Z","iopub.status.idle":"2024-09-30T07:07:31.126110Z","shell.execute_reply.started":"2024-09-30T07:07:27.390161Z","shell.execute_reply":"2024-09-30T07:07:31.124774Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:07:39.687857Z","iopub.execute_input":"2024-09-30T07:07:39.688842Z","iopub.status.idle":"2024-09-30T07:07:40.138851Z","shell.execute_reply.started":"2024-09-30T07:07:39.688798Z","shell.execute_reply":"2024-09-30T07:07:40.137711Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create PyTorch Dataset\nclass TfidfDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_dataset = TfidfDataset(X_train_tensor, y_train_tensor)\nval_dataset = TfidfDataset(X_val_tensor, y_val_tensor)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:08:05.873362Z","iopub.execute_input":"2024-09-30T07:08:05.873803Z","iopub.status.idle":"2024-09-30T07:08:08.691102Z","shell.execute_reply.started":"2024-09-30T07:08:05.873759Z","shell.execute_reply":"2024-09-30T07:08:08.689997Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nclass LogisticRegressionClassifier(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionClassifier, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:10:54.403296Z","iopub.execute_input":"2024-09-30T07:10:54.404244Z","iopub.status.idle":"2024-09-30T07:10:54.411777Z","shell.execute_reply.started":"2024-09-30T07:10:54.404186Z","shell.execute_reply":"2024-09-30T07:10:54.410478Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# Model Parameters\ninput_dim = X_train.shape[1]\noutput_dim = len(label_encoder.classes_)\n\n# Instantiate the model\nmodel = LogisticRegressionClassifier(input_dim, output_dim)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.005)\n\n# Move model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\nnum_epochs = 50  \npatience = 5  # Number of epochs with no improvement after which training will be stopped\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\n# Training loop with early stopping\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        predictions = model(X_batch)\n\n        # Calculate loss\n        loss = criterion(predictions, y_batch)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate epoch loss\n        epoch_loss += loss.item()\n\n    # Validation phase\n    model.eval()\n    val_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            predictions = model(X_batch)\n            loss = criterion(predictions, y_batch)\n            val_loss += loss.item()\n\n            # Calculate accuracy\n            _, predicted_classes = torch.max(predictions, 1)\n            correct_predictions += (predicted_classes == y_batch).sum().item()\n            total_predictions += y_batch.size(0)\n\n    val_acc = correct_predictions / total_predictions\n    avg_val_loss = val_loss / len(val_loader)\n\n    # Print the results for the current epoch\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss / len(train_loader):.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        # You can also save the model if it has the best validation loss so far\n        best_model_state = model.state_dict()\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(f'Early stopping triggered after {epoch + 1} epochs.')\n        break\n\n# Load the best model state if needed\nmodel.load_state_dict(best_model_state)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:13:24.398358Z","iopub.execute_input":"2024-09-30T07:13:24.398775Z","iopub.status.idle":"2024-09-30T07:13:37.498149Z","shell.execute_reply.started":"2024-09-30T07:13:24.398736Z","shell.execute_reply":"2024-09-30T07:13:37.497107Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch [1/50], Training Loss: 2.5063, Validation Loss: 2.1089, Validation Accuracy: 0.6629\nEpoch [2/50], Training Loss: 1.7015, Validation Loss: 1.6734, Validation Accuracy: 0.6867\nEpoch [3/50], Training Loss: 1.2818, Validation Loss: 1.4430, Validation Accuracy: 0.6947\nEpoch [4/50], Training Loss: 1.0336, Validation Loss: 1.3097, Validation Accuracy: 0.6987\nEpoch [5/50], Training Loss: 0.8700, Validation Loss: 1.2250, Validation Accuracy: 0.7029\nEpoch [6/50], Training Loss: 0.7538, Validation Loss: 1.1674, Validation Accuracy: 0.7027\nEpoch [7/50], Training Loss: 0.6645, Validation Loss: 1.1264, Validation Accuracy: 0.7021\nEpoch [8/50], Training Loss: 0.5948, Validation Loss: 1.0984, Validation Accuracy: 0.6997\nEpoch [9/50], Training Loss: 0.5382, Validation Loss: 1.0769, Validation Accuracy: 0.6989\nEpoch [10/50], Training Loss: 0.4904, Validation Loss: 1.0613, Validation Accuracy: 0.6987\nEpoch [11/50], Training Loss: 0.4506, Validation Loss: 1.0497, Validation Accuracy: 0.6960\nEpoch [12/50], Training Loss: 0.4159, Validation Loss: 1.0415, Validation Accuracy: 0.6942\nEpoch [13/50], Training Loss: 0.3861, Validation Loss: 1.0362, Validation Accuracy: 0.6944\nEpoch [14/50], Training Loss: 0.3597, Validation Loss: 1.0318, Validation Accuracy: 0.6955\nEpoch [15/50], Training Loss: 0.3362, Validation Loss: 1.0302, Validation Accuracy: 0.6923\nEpoch [16/50], Training Loss: 0.3157, Validation Loss: 1.0295, Validation Accuracy: 0.6915\nEpoch [17/50], Training Loss: 0.2974, Validation Loss: 1.0297, Validation Accuracy: 0.6923\nEpoch [18/50], Training Loss: 0.2806, Validation Loss: 1.0315, Validation Accuracy: 0.6910\nEpoch [19/50], Training Loss: 0.2659, Validation Loss: 1.0340, Validation Accuracy: 0.6870\nEpoch [20/50], Training Loss: 0.2525, Validation Loss: 1.0368, Validation Accuracy: 0.6875\nEpoch [21/50], Training Loss: 0.2405, Validation Loss: 1.0405, Validation Accuracy: 0.6865\nEarly stopping triggered after 21 epochs.\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:17:16.709543Z","iopub.execute_input":"2024-09-30T07:17:16.710353Z","iopub.status.idle":"2024-09-30T07:17:16.714355Z","shell.execute_reply.started":"2024-09-30T07:17:16.710308Z","shell.execute_reply":"2024-09-30T07:17:16.713363Z"}}},{"cell_type":"code","source":"# Switch model to evaluation mode\nmodel.eval()\n\n# Initialize lists to store the predictions and true labels\nall_predictions = []\nall_labels = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        # Get model predictions\n        outputs = model(X_batch)\n        _, predicted = torch.max(outputs, 1)\n        \n        # Store predictions and labels\n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.cpu().numpy())\n\n# Convert to NumPy arrays for evaluation\nall_predictions = np.array(all_predictions)\nall_labels = np.array(all_labels)\n\n# Calculate accuracy\naccuracy = accuracy_score(all_labels, all_predictions)\n\n# Calculate precision, recall, and F1-score\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n\n# Print evaluation results\nprint(f'Validation Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:17:22.374130Z","iopub.execute_input":"2024-09-30T07:17:22.375117Z","iopub.status.idle":"2024-09-30T07:17:22.473323Z","shell.execute_reply.started":"2024-09-30T07:17:22.375072Z","shell.execute_reply":"2024-09-30T07:17:22.472290Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.6865\nPrecision: 0.6949\nRecall: 0.6865\nF1 Score: 0.6882\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize lists to store the predictions and true labels\nall_predictions = []\nall_labels = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for X_batch, y_batch in val_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n        # Get model predictions\n        outputs = model(X_batch)\n        _, predicted = torch.max(outputs, 1)\n\n        # Store predictions and labels\n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(y_batch.cpu().numpy())\n\n# Convert to NumPy arrays for evaluation\nall_predictions = np.array(all_predictions)\nall_labels = np.array(all_labels)\n\n# Convert label encoder classes to a list of strings\ntarget_names = list(map(str, label_encoder.classes_))\n\n# Generate a classification report\nreport = classification_report(all_labels, all_predictions, target_names=target_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T07:21:32.065543Z","iopub.execute_input":"2024-09-30T07:21:32.066526Z","iopub.status.idle":"2024-09-30T07:21:32.170697Z","shell.execute_reply.started":"2024-09-30T07:21:32.066480Z","shell.execute_reply":"2024-09-30T07:21:32.169694Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.53      0.52      0.52       151\n           1       0.65      0.62      0.63       202\n           2       0.63      0.59      0.61       195\n           3       0.59      0.64      0.61       183\n           4       0.71      0.61      0.66       205\n           5       0.78      0.78      0.78       215\n           6       0.73      0.69      0.71       193\n           7       0.47      0.73      0.57       196\n           8       0.68      0.69      0.69       168\n           9       0.84      0.80      0.82       211\n          10       0.92      0.85      0.88       198\n          11       0.85      0.75      0.79       201\n          12       0.58      0.63      0.60       202\n          13       0.78      0.80      0.79       194\n          14       0.76      0.74      0.75       189\n          15       0.76      0.75      0.76       202\n          16       0.65      0.67      0.66       188\n          17       0.79      0.74      0.77       182\n          18       0.59      0.60      0.59       159\n          19       0.46      0.37      0.41       136\n\n    accuracy                           0.69      3770\n   macro avg       0.69      0.68      0.68      3770\nweighted avg       0.69      0.69      0.69      3770\n\n","output_type":"stream"}]}]}